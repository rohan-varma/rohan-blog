{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "from load_mnist import MNIST_Loader\n",
    "from error_validation import *\n",
    "print \"loaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ x_{i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "a = MNIST_Loader()\n",
    "X_train, y_train = a.load_mnist('../data')\n",
    "X_test, y_test = a.load_mnist('../data', 't10k')\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(n_output=10,\n",
    "                  n_features=X_train.shape[1],\n",
    "                  n_hidden=100,\n",
    "                  l2=0.1,\n",
    "                  epochs=1000,\n",
    "                  learning_rate=0.001,\n",
    "                  momentum_const=0.5,\n",
    "                  decay_rate=0.00001,\n",
    "                  dropout=True,\n",
    "                  minibatch_size=500,\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 11.4863186906\n",
      "Training Accuracy: 72.3316666667\n",
      "Epoch: 2\n",
      "Loss: 9.96101675805\n",
      "Training Accuracy: 77.6033333333\n",
      "Epoch: 3\n",
      "Loss: 8.79242232026\n",
      "Training Accuracy: 81.3816666667\n",
      "Epoch: 4\n",
      "Loss: 7.67417013288\n",
      "Training Accuracy: 81.8216666667\n",
      "Epoch: 5\n",
      "Loss: 6.86434756793\n",
      "Training Accuracy: 82.4183333333\n",
      "Epoch: 6\n",
      "Loss: 6.21540978285\n",
      "Training Accuracy: 84.2866666667\n",
      "Epoch: 7\n",
      "Loss: 5.48939117542\n",
      "Training Accuracy: 83.8516666667\n",
      "Epoch: 8\n",
      "Loss: 5.02466623227\n",
      "Training Accuracy: 84.2883333333\n",
      "Epoch: 9\n",
      "Loss: 4.38899386563\n",
      "Training Accuracy: 85.065\n",
      "Epoch: 10\n",
      "Loss: 4.1225288191\n",
      "Training Accuracy: 85.8333333333\n",
      "Epoch: 11\n",
      "Loss: 3.60661741608\n",
      "Training Accuracy: 85.6433333333\n",
      "Epoch: 12\n",
      "Loss: 3.32130372311\n",
      "Training Accuracy: 85.415\n",
      "Epoch: 13\n",
      "Loss: 3.12704554356\n",
      "Training Accuracy: 86.6683333333\n",
      "Epoch: 14\n",
      "Loss: 2.95022814615\n",
      "Training Accuracy: 85.2333333333\n",
      "Epoch: 15\n",
      "Loss: 2.78257690437\n",
      "Training Accuracy: 86.695\n",
      "Epoch: 16\n",
      "Loss: 2.59735603243\n",
      "Training Accuracy: 87.62\n",
      "Epoch: 17\n",
      "Loss: 2.57316813042\n",
      "Training Accuracy: 86.8433333333\n",
      "Epoch: 18\n",
      "Loss: 2.35519925188\n",
      "Training Accuracy: 87.15\n",
      "Epoch: 19\n",
      "Loss: 2.30072864861\n",
      "Training Accuracy: 86.3716666667\n",
      "Epoch: 20\n",
      "Loss: 2.09264247987\n",
      "Training Accuracy: 86.6866666667\n",
      "Epoch: 21\n",
      "Loss: 2.02089140513\n",
      "Training Accuracy: 87.43\n",
      "Epoch: 22\n",
      "Loss: 1.87384529019\n",
      "Training Accuracy: 87.545\n",
      "Epoch: 23\n",
      "Loss: 2.01161874031\n",
      "Training Accuracy: 87.5266666667\n",
      "Epoch: 24\n",
      "Loss: 1.93545063244\n",
      "Training Accuracy: 86.9383333333\n",
      "Epoch: 25\n",
      "Loss: 1.97518485957\n",
      "Training Accuracy: 87.745\n",
      "Epoch: 26\n",
      "Loss: 1.84365427507\n",
      "Training Accuracy: 87.9216666667\n",
      "Epoch: 27\n",
      "Loss: 1.76063144692\n",
      "Training Accuracy: 86.6066666667\n",
      "Epoch: 28\n",
      "Loss: 1.92910471683\n",
      "Training Accuracy: 87.2566666667\n",
      "Epoch: 29\n",
      "Loss: 1.83810432939\n",
      "Training Accuracy: 86.465\n",
      "Epoch: 30\n",
      "Loss: 1.94834966733\n",
      "Training Accuracy: 87.0866666667\n",
      "Epoch: 31\n",
      "Loss: 1.79249844874\n",
      "Training Accuracy: 86.59\n",
      "Epoch: 32\n",
      "Loss: 1.94786271409\n",
      "Training Accuracy: 87.1583333333\n",
      "Epoch: 33\n",
      "Loss: 1.84410236577\n",
      "Training Accuracy: 87.4983333333\n",
      "Epoch: 34\n",
      "Loss: 1.78089678144\n",
      "Training Accuracy: 86.7066666667\n",
      "Epoch: 35\n",
      "Loss: 1.90232460126\n",
      "Training Accuracy: 87.355\n",
      "Epoch: 36\n",
      "Loss: 1.66279323437\n",
      "Training Accuracy: 87.3533333333\n",
      "Epoch: 37\n",
      "Loss: 1.8495530377\n",
      "Training Accuracy: 87.645\n",
      "Epoch: 38\n",
      "Loss: 1.83763016732\n",
      "Training Accuracy: 87.75\n",
      "Epoch: 39\n",
      "Loss: 1.73801953701\n",
      "Training Accuracy: 87.835\n",
      "Epoch: 40\n",
      "Loss: 1.76080049355\n",
      "Training Accuracy: 87.8083333333\n",
      "Epoch: 41\n",
      "Loss: 1.72005611867\n",
      "Training Accuracy: 87.7133333333\n",
      "Epoch: 42\n",
      "Loss: 1.64463986957\n",
      "Training Accuracy: 87.985\n",
      "Epoch: 43\n",
      "Loss: 1.78245046971\n",
      "Training Accuracy: 88.0116666667\n",
      "Epoch: 44\n",
      "Loss: 1.68990969433\n",
      "Training Accuracy: 88.2966666667\n",
      "Epoch: 45\n",
      "Loss: 1.65815029372\n",
      "Training Accuracy: 88.3233333333\n",
      "Epoch: 46\n",
      "Loss: 1.76690587667\n",
      "Training Accuracy: 87.4666666667\n",
      "Epoch: 47\n",
      "Loss: 1.65396960715\n",
      "Training Accuracy: 87.97\n",
      "Epoch: 48\n",
      "Loss: 1.67016989765\n",
      "Training Accuracy: 87.875\n",
      "Epoch: 49\n",
      "Loss: 1.63500896345\n",
      "Training Accuracy: 87.9\n",
      "Epoch: 50\n",
      "Loss: 1.74454094906\n",
      "Training Accuracy: 87.7616666667\n",
      "Epoch: 51\n",
      "Loss: 1.73249982236\n",
      "Training Accuracy: 87.51\n",
      "Epoch: 52\n",
      "Loss: 1.69720026093\n",
      "Training Accuracy: 87.78\n",
      "Epoch: 53\n",
      "Loss: 1.67204144533\n",
      "Training Accuracy: 87.665\n",
      "Epoch: 54\n",
      "Loss: 1.7915602708\n",
      "Training Accuracy: 87.8666666667\n",
      "Epoch: 55\n",
      "Loss: 1.61625530496\n",
      "Training Accuracy: 87.3866666667\n",
      "Epoch: 56\n",
      "Loss: 1.75566330926\n",
      "Training Accuracy: 87.275\n",
      "Epoch: 57\n",
      "Loss: 1.7323454866\n",
      "Training Accuracy: 88.135\n",
      "Epoch: 58\n",
      "Loss: 1.68533418371\n",
      "Training Accuracy: 87.4633333333\n",
      "Epoch: 59\n",
      "Loss: 1.57664432815\n",
      "Training Accuracy: 87.4666666667\n",
      "Epoch: 60\n",
      "Loss: 1.58079258499\n",
      "Training Accuracy: 87.9816666667\n",
      "Epoch: 61\n",
      "Loss: 1.77328064955\n",
      "Training Accuracy: 87.7716666667\n",
      "Epoch: 62\n",
      "Loss: 1.77379542635\n",
      "Training Accuracy: 87.4083333333\n",
      "Epoch: 63\n",
      "Loss: 1.78901506086\n",
      "Training Accuracy: 87.72\n",
      "Epoch: 64\n",
      "Loss: 1.76184927807\n",
      "Training Accuracy: 87.8633333333\n",
      "Epoch: 65\n",
      "Loss: 1.73539331149\n",
      "Training Accuracy: 87.1466666667\n",
      "Epoch: 66\n",
      "Loss: 1.73247109933\n",
      "Training Accuracy: 87.7783333333\n",
      "Epoch: 67\n",
      "Loss: 1.72600392915\n",
      "Training Accuracy: 87.73\n",
      "Epoch: 68\n",
      "Loss: 1.68576605368\n",
      "Training Accuracy: 88.0966666667\n",
      "Epoch: 69\n",
      "Loss: 1.71021887363\n",
      "Training Accuracy: 87.475\n",
      "Epoch: 70\n",
      "Loss: 1.80132104645\n",
      "Training Accuracy: 87.3066666667\n",
      "Epoch: 71\n",
      "Loss: 1.58748218452\n",
      "Training Accuracy: 87.33\n",
      "Epoch: 72\n",
      "Loss: 1.71121343781\n",
      "Training Accuracy: 87.7733333333\n",
      "Epoch: 73\n",
      "Loss: 1.73941132835\n",
      "Training Accuracy: 87.545\n",
      "Epoch: 74\n",
      "Loss: 1.71702767111\n",
      "Training Accuracy: 86.9433333333\n",
      "Epoch: 75\n",
      "Loss: 1.76708665137\n",
      "Training Accuracy: 87.195\n",
      "Epoch: 76\n",
      "Loss: 1.75792549956\n",
      "Training Accuracy: 87.025\n",
      "Epoch: 77\n",
      "Loss: 1.73419107272\n",
      "Training Accuracy: 86.4333333333\n",
      "Epoch: 78\n",
      "Loss: 1.73454413932\n",
      "Training Accuracy: 87.3416666667\n",
      "Epoch: 79\n",
      "Loss: 1.62456091088\n",
      "Training Accuracy: 87.3566666667\n",
      "Epoch: 80\n",
      "Loss: 1.60979252225\n",
      "Training Accuracy: 87.5433333333\n",
      "Epoch: 81\n",
      "Loss: 1.66751965039\n",
      "Training Accuracy: 87.3966666667\n",
      "Epoch: 82\n",
      "Loss: 1.62250768748\n",
      "Training Accuracy: 87.755\n",
      "Epoch: 83\n",
      "Loss: 1.64304659123\n",
      "Training Accuracy: 87.8833333333\n",
      "Epoch: 84\n",
      "Loss: 1.75796352961\n",
      "Training Accuracy: 87.7316666667\n",
      "Epoch: 85\n",
      "Loss: 1.74790936435\n",
      "Training Accuracy: 87.75\n",
      "Epoch: 86\n",
      "Loss: 1.71049916489\n",
      "Training Accuracy: 87.96\n",
      "Epoch: 87\n",
      "Loss: 1.68268786859\n",
      "Training Accuracy: 87.8516666667\n",
      "Epoch: 88\n",
      "Loss: 1.65806751693\n",
      "Training Accuracy: 87.5216666667\n",
      "Epoch: 89\n",
      "Loss: 1.62831148205\n",
      "Training Accuracy: 87.5966666667\n",
      "Epoch: 90\n",
      "Loss: 1.70275808421\n",
      "Training Accuracy: 88.0683333333\n",
      "Epoch: 91\n",
      "Loss: 1.69189146428\n",
      "Training Accuracy: 87.165\n",
      "Epoch: 92\n",
      "Loss: 1.73934576475\n",
      "Training Accuracy: 87.435\n",
      "Epoch: 93\n",
      "Loss: 1.7055709905\n",
      "Training Accuracy: 86.8566666667\n",
      "Epoch: 94\n",
      "Loss: 1.64538390203\n",
      "Training Accuracy: 87.4316666667\n",
      "Epoch: 95\n",
      "Loss: 1.6471513812\n",
      "Training Accuracy: 87.5866666667\n",
      "Epoch: 96\n",
      "Loss: 1.64755251804\n",
      "Training Accuracy: 87.5633333333\n",
      "Epoch: 97\n",
      "Loss: 1.75904170597\n",
      "Training Accuracy: 87.975\n",
      "Epoch: 98\n",
      "Loss: 1.65445851563\n",
      "Training Accuracy: 87.76\n",
      "Epoch: 99\n",
      "Loss: 1.68180590857\n",
      "Training Accuracy: 87.685\n",
      "Epoch: 100\n",
      "Loss: 1.59884659944\n",
      "Training Accuracy: 88.065\n",
      "Epoch: 101\n",
      "Loss: 1.80940465397\n",
      "Training Accuracy: 88.3283333333\n",
      "Epoch: 102\n",
      "Loss: 1.71799530627\n",
      "Training Accuracy: 87.765\n",
      "Epoch: 103\n",
      "Loss: 1.62966672597\n",
      "Training Accuracy: 88.1233333333\n",
      "Epoch: 104\n",
      "Loss: 1.79134246078\n",
      "Training Accuracy: 87.845\n",
      "Epoch: 105\n",
      "Loss: 1.68402047463\n",
      "Training Accuracy: 88.4116666667\n",
      "Epoch: 106\n",
      "Loss: 1.6719334414\n",
      "Training Accuracy: 87.945\n",
      "Epoch: 107\n",
      "Loss: 1.65044263275\n",
      "Training Accuracy: 87.8966666667\n",
      "Epoch: 108\n",
      "Loss: 1.68510469234\n",
      "Training Accuracy: 87.8666666667\n",
      "Epoch: 109\n",
      "Loss: 1.74278915766\n",
      "Training Accuracy: 87.38\n",
      "Epoch: 110\n",
      "Loss: 1.57187753705\n",
      "Training Accuracy: 88.365\n",
      "Epoch: 111\n",
      "Loss: 1.56701933243\n",
      "Training Accuracy: 88.2083333333\n",
      "Epoch: 112\n",
      "Loss: 1.71344277534\n",
      "Training Accuracy: 87.6683333333\n",
      "Epoch: 113\n",
      "Loss: 1.71536400066\n",
      "Training Accuracy: 87.5233333333\n",
      "Epoch: 114\n",
      "Loss: 1.72853814863\n",
      "Training Accuracy: 87.74\n",
      "Epoch: 115\n",
      "Loss: 1.67555124781\n",
      "Training Accuracy: 88.29\n",
      "Epoch: 116\n",
      "Loss: 1.66977894397\n",
      "Training Accuracy: 87.77\n",
      "Epoch: 117\n",
      "Loss: 1.59997567052\n",
      "Training Accuracy: 88.13\n",
      "Epoch: 118\n",
      "Loss: 1.56667706585\n",
      "Training Accuracy: 88.3216666667\n",
      "Epoch: 119\n",
      "Loss: 1.69199487892\n",
      "Training Accuracy: 87.675\n",
      "Epoch: 120\n",
      "Loss: 1.64918814992\n",
      "Training Accuracy: 87.82\n",
      "Epoch: 121\n",
      "Loss: 1.71272130487\n",
      "Training Accuracy: 87.3766666667\n",
      "Epoch: 122\n",
      "Loss: 1.6827313301\n",
      "Training Accuracy: 87.9466666667\n",
      "Epoch: 123\n",
      "Loss: 1.64034417477\n",
      "Training Accuracy: 88.0366666667\n",
      "Epoch: 124\n",
      "Loss: 1.62346329628\n",
      "Training Accuracy: 88.4516666667\n",
      "Epoch: 125\n",
      "Loss: 1.57549115893\n",
      "Training Accuracy: 88.0033333333\n",
      "Epoch: 126\n",
      "Loss: 1.8167715978\n",
      "Training Accuracy: 88.0616666667\n",
      "Epoch: 127\n",
      "Loss: 1.64124277657\n",
      "Training Accuracy: 88.4733333333\n",
      "Epoch: 128\n",
      "Loss: 1.67556210752\n",
      "Training Accuracy: 88.305\n",
      "Epoch: 129\n",
      "Loss: 1.64397357127\n",
      "Training Accuracy: 87.805\n",
      "Epoch: 130\n",
      "Loss: 1.63784787799\n",
      "Training Accuracy: 88.4366666667\n",
      "Epoch: 131\n",
      "Loss: 1.57709003579\n",
      "Training Accuracy: 88.4616666667\n",
      "Epoch: 132\n",
      "Loss: 1.62461939449\n",
      "Training Accuracy: 87.815\n",
      "Epoch: 133\n",
      "Loss: 1.59906653206\n",
      "Training Accuracy: 88.2816666667\n",
      "Epoch: 134\n",
      "Loss: 1.67072125451\n",
      "Training Accuracy: 88.47\n",
      "Epoch: 135\n",
      "Loss: 1.44600718353\n",
      "Training Accuracy: 88.0666666667\n",
      "Epoch: 136\n",
      "Loss: 1.69540839261\n",
      "Training Accuracy: 88.5366666667\n",
      "Epoch: 137\n",
      "Loss: 1.6633212559\n",
      "Training Accuracy: 88.3383333333\n",
      "Epoch: 138\n",
      "Loss: 1.6385122823\n",
      "Training Accuracy: 88.045\n",
      "Epoch: 139\n",
      "Loss: 1.62373064308\n",
      "Training Accuracy: 87.9883333333\n",
      "Epoch: 140\n",
      "Loss: 1.7188473401\n",
      "Training Accuracy: 87.835\n",
      "Epoch: 141\n",
      "Loss: 1.59578334279\n",
      "Training Accuracy: 87.9383333333\n",
      "Epoch: 142\n",
      "Loss: 1.6205834002\n",
      "Training Accuracy: 87.7633333333\n",
      "Epoch: 143\n",
      "Loss: 1.53972807362\n",
      "Training Accuracy: 88.45\n",
      "Epoch: 144\n",
      "Loss: 1.61791361723\n",
      "Training Accuracy: 88.2283333333\n",
      "Epoch: 145\n",
      "Loss: 1.59903898166\n",
      "Training Accuracy: 87.5416666667\n",
      "Epoch: 146\n",
      "Loss: 1.70635194831\n",
      "Training Accuracy: 88.8116666667\n",
      "Epoch: 147\n",
      "Loss: 1.51059291801\n",
      "Training Accuracy: 87.4933333333\n",
      "Epoch: 148\n",
      "Loss: 1.54842424316\n",
      "Training Accuracy: 87.8733333333\n",
      "Epoch: 149\n",
      "Loss: 1.57416683316\n",
      "Training Accuracy: 88.2983333333\n",
      "Epoch: 150\n",
      "Loss: 1.63882175431\n",
      "Training Accuracy: 87.5683333333\n",
      "Epoch: 151\n",
      "Loss: 1.63482910931\n",
      "Training Accuracy: 87.59\n",
      "Epoch: 152\n",
      "Loss: 1.57304264772\n",
      "Training Accuracy: 88.3866666667\n",
      "Epoch: 153\n",
      "Loss: 1.57125781903\n",
      "Training Accuracy: 88.1333333333\n",
      "Epoch: 154\n",
      "Loss: 1.61383224199\n",
      "Training Accuracy: 88.4033333333\n",
      "Epoch: 155\n",
      "Loss: 1.5379196737\n",
      "Training Accuracy: 88.7283333333\n",
      "Epoch: 156\n",
      "Loss: 1.5760986762\n",
      "Training Accuracy: 87.8683333333\n",
      "Epoch: 157\n",
      "Loss: 1.73222045467\n",
      "Training Accuracy: 88.0466666667\n",
      "Epoch: 158\n",
      "Loss: 1.69593370109\n",
      "Training Accuracy: 88.0416666667\n",
      "Epoch: 159\n",
      "Loss: 1.64485595642\n",
      "Training Accuracy: 88.4033333333\n",
      "Epoch: 160\n",
      "Loss: 1.47459065443\n",
      "Training Accuracy: 88.1766666667\n",
      "Epoch: 161\n",
      "Loss: 1.68215067577\n",
      "Training Accuracy: 88.1383333333\n",
      "Epoch: 162\n",
      "Loss: 1.62699486243\n",
      "Training Accuracy: 88.075\n",
      "Epoch: 163\n",
      "Loss: 1.58008996397\n",
      "Training Accuracy: 87.9883333333\n",
      "Epoch: 164\n",
      "Loss: 1.53023831517\n",
      "Training Accuracy: 89.215\n",
      "Epoch: 165\n",
      "Loss: 1.49405977069\n",
      "Training Accuracy: 88.3616666667\n",
      "Epoch: 166\n",
      "Loss: 1.54378637133\n",
      "Training Accuracy: 87.7583333333\n",
      "Epoch: 167\n",
      "Loss: 1.52368425456\n",
      "Training Accuracy: 88.31\n",
      "Epoch: 168\n",
      "Loss: 1.6324398057\n",
      "Training Accuracy: 88.5533333333\n",
      "Epoch: 169\n",
      "Loss: 1.5021500182\n",
      "Training Accuracy: 88.2566666667\n",
      "Epoch: 170\n",
      "Loss: 1.49524623782\n",
      "Training Accuracy: 88.2\n",
      "Epoch: 171\n",
      "Loss: 1.73810625162\n",
      "Training Accuracy: 87.8733333333\n",
      "Epoch: 172\n",
      "Loss: 1.62438087663\n",
      "Training Accuracy: 88.03\n",
      "Epoch: 173\n",
      "Loss: 1.49051825743\n",
      "Training Accuracy: 88.2283333333\n",
      "Epoch: 174\n",
      "Loss: 1.55297874393\n",
      "Training Accuracy: 88.2366666667\n",
      "Epoch: 175\n",
      "Loss: 1.50974215173\n",
      "Training Accuracy: 88.5933333333\n",
      "Epoch: 176\n",
      "Loss: 1.650593541\n",
      "Training Accuracy: 88.175\n",
      "Epoch: 177\n",
      "Loss: 1.58546716473\n",
      "Training Accuracy: 88.0916666667\n",
      "Epoch: 178\n",
      "Loss: 1.55737681381\n",
      "Training Accuracy: 88.9083333333\n",
      "Epoch: 179\n",
      "Loss: 1.51195086901\n",
      "Training Accuracy: 88.3\n",
      "Epoch: 180\n",
      "Loss: 1.64020297253\n",
      "Training Accuracy: 88.4066666667\n",
      "Epoch: 181\n",
      "Loss: 1.50509201126\n",
      "Training Accuracy: 88.8816666667\n",
      "Epoch: 182\n",
      "Loss: 1.56817417097\n",
      "Training Accuracy: 88.36\n",
      "Epoch: 183\n",
      "Loss: 1.38196863479\n",
      "Training Accuracy: 87.8133333333\n",
      "Epoch: 184\n",
      "Loss: 1.5552258854\n",
      "Training Accuracy: 88.775\n",
      "Epoch: 185\n",
      "Loss: 1.54264972804\n",
      "Training Accuracy: 88.705\n",
      "Epoch: 186\n",
      "Loss: 1.51080608173\n",
      "Training Accuracy: 88.6466666667\n",
      "Epoch: 187\n",
      "Loss: 1.48761685368\n",
      "Training Accuracy: 88.7283333333\n",
      "Epoch: 188\n",
      "Loss: 1.57839533405\n",
      "Training Accuracy: 88.36\n",
      "Epoch: 189\n",
      "Loss: 1.56343553786\n",
      "Training Accuracy: 88.355\n",
      "Epoch: 190\n",
      "Loss: 1.49730909604\n",
      "Training Accuracy: 88.575\n",
      "Epoch: 191\n",
      "Loss: 1.59373046432\n",
      "Training Accuracy: 89.07\n",
      "Epoch: 192\n",
      "Loss: 1.63377318272\n",
      "Training Accuracy: 88.55\n",
      "Epoch: 193\n",
      "Loss: 1.50038560896\n",
      "Training Accuracy: 89.02\n",
      "Epoch: 194\n",
      "Loss: 1.586914044\n",
      "Training Accuracy: 88.4566666667\n",
      "Epoch: 195\n",
      "Loss: 1.55216493769\n",
      "Training Accuracy: 87.935\n",
      "Epoch: 196\n",
      "Loss: 1.63170811503\n",
      "Training Accuracy: 89.0066666667\n",
      "Epoch: 197\n",
      "Loss: 1.38043054408\n",
      "Training Accuracy: 89.105\n",
      "Epoch: 198\n",
      "Loss: 1.49782733828\n",
      "Training Accuracy: 89.04\n",
      "Epoch: 199\n",
      "Loss: 1.49906733294\n",
      "Training Accuracy: 88.4133333333\n",
      "Epoch: 200\n",
      "Loss: 1.43160671918\n",
      "Training Accuracy: 88.5916666667\n",
      "Epoch: 201\n",
      "Loss: 1.59586355714\n",
      "Training Accuracy: 88.83\n",
      "Epoch: 202\n",
      "Loss: 1.45699385277\n",
      "Training Accuracy: 89.18\n",
      "Epoch: 203\n",
      "Loss: 1.6146873644\n",
      "Training Accuracy: 88.41\n",
      "Epoch: 204\n",
      "Loss: 1.63612013606\n",
      "Training Accuracy: 87.895\n",
      "Epoch: 205\n",
      "Loss: 1.56511449527\n",
      "Training Accuracy: 88.05\n",
      "Epoch: 206\n",
      "Loss: 1.57384165032\n",
      "Training Accuracy: 88.7\n",
      "Epoch: 207\n",
      "Loss: 1.4837942977\n",
      "Training Accuracy: 88.855\n",
      "Epoch: 208\n",
      "Loss: 1.4922999129\n",
      "Training Accuracy: 88.5016666667\n",
      "Epoch: 209\n",
      "Loss: 1.62967450684\n",
      "Training Accuracy: 88.65\n",
      "Epoch: 210\n",
      "Loss: 1.516330897\n",
      "Training Accuracy: 88.1083333333\n",
      "Epoch: 211\n",
      "Loss: 1.51200037331\n",
      "Training Accuracy: 89.2583333333\n",
      "Epoch: 212\n",
      "Loss: 1.48524052885\n",
      "Training Accuracy: 89.0233333333\n",
      "Epoch: 213\n",
      "Loss: 1.49857380213\n",
      "Training Accuracy: 88.21\n",
      "Epoch: 214\n",
      "Loss: 1.53957699844\n",
      "Training Accuracy: 88.4533333333\n",
      "Epoch: 215\n",
      "Loss: 1.53581624681\n",
      "Training Accuracy: 88.265\n",
      "Epoch: 216\n",
      "Loss: 1.45776591613\n",
      "Training Accuracy: 89.01\n",
      "Epoch: 217\n",
      "Loss: 1.54923063343\n",
      "Training Accuracy: 88.8566666667\n",
      "Epoch: 218\n",
      "Loss: 1.52405984907\n",
      "Training Accuracy: 89.035\n",
      "Epoch: 219\n",
      "Loss: 1.63505239361\n",
      "Training Accuracy: 88.4433333333\n",
      "Epoch: 220\n",
      "Loss: 1.44562030278\n",
      "Training Accuracy: 88.8883333333\n",
      "Epoch: 221\n",
      "Loss: 1.43844339375\n",
      "Training Accuracy: 89.325\n",
      "Epoch: 222\n",
      "Loss: 1.4332033572\n",
      "Training Accuracy: 89.62\n",
      "Epoch: 223\n",
      "Loss: 1.35584682986\n",
      "Training Accuracy: 89.3766666667\n",
      "Epoch: 224\n",
      "Loss: 1.39690730964\n",
      "Training Accuracy: 88.59\n",
      "Epoch: 225\n",
      "Loss: 1.55546766707\n",
      "Training Accuracy: 87.9216666667\n",
      "Epoch: 226\n",
      "Loss: 1.51792671001\n",
      "Training Accuracy: 88.645\n",
      "Epoch: 227\n",
      "Loss: 1.50737584108\n",
      "Training Accuracy: 89.01\n",
      "Epoch: 228\n",
      "Loss: 1.47325708201\n",
      "Training Accuracy: 88.7216666667\n",
      "Epoch: 229\n",
      "Loss: 1.35951254035\n",
      "Training Accuracy: 89.2\n",
      "Epoch: 230\n",
      "Loss: 1.51441271898\n",
      "Training Accuracy: 88.4616666667\n",
      "Epoch: 231\n",
      "Loss: 1.49489512274\n",
      "Training Accuracy: 88.95\n",
      "Epoch: 232\n",
      "Loss: 1.51721413425\n",
      "Training Accuracy: 88.8366666667\n",
      "Epoch: 233\n",
      "Loss: 1.41989461423\n",
      "Training Accuracy: 88.6916666667\n",
      "Epoch: 234\n",
      "Loss: 1.46927586467\n",
      "Training Accuracy: 89.0266666667\n",
      "Epoch: 235\n",
      "Loss: 1.53962394814\n",
      "Training Accuracy: 88.9316666667\n",
      "Epoch: 236\n",
      "Loss: 1.35596811945\n",
      "Training Accuracy: 88.8833333333\n",
      "Epoch: 237\n",
      "Loss: 1.45343823317\n",
      "Training Accuracy: 88.7133333333\n",
      "Epoch: 238\n",
      "Loss: 1.53752593247\n",
      "Training Accuracy: 89.1283333333\n",
      "Epoch: 239\n",
      "Loss: 1.46795703137\n",
      "Training Accuracy: 89.0816666667\n",
      "Epoch: 240\n",
      "Loss: 1.44756689251\n",
      "Training Accuracy: 89.015\n",
      "Epoch: 241\n",
      "Loss: 1.54716424105\n",
      "Training Accuracy: 89.155\n",
      "Epoch: 242\n",
      "Loss: 1.44735549683\n",
      "Training Accuracy: 88.9366666667\n",
      "Epoch: 243\n",
      "Loss: 1.47075244619\n",
      "Training Accuracy: 88.875\n",
      "Epoch: 244\n",
      "Loss: 1.42591474285\n",
      "Training Accuracy: 88.89\n",
      "Epoch: 245\n",
      "Loss: 1.46600726837\n",
      "Training Accuracy: 88.8183333333\n",
      "Epoch: 246\n",
      "Loss: 1.46771086397\n",
      "Training Accuracy: 88.965\n",
      "Epoch: 247\n",
      "Loss: 1.37388064239\n",
      "Training Accuracy: 88.94\n",
      "Epoch: 248\n",
      "Loss: 1.35874354676\n",
      "Training Accuracy: 88.5266666667\n",
      "Epoch: 249\n",
      "Loss: 1.45577592227\n",
      "Training Accuracy: 89.545\n",
      "Epoch: 250\n",
      "Loss: 1.46390033938\n",
      "Training Accuracy: 88.4\n",
      "Epoch: 251\n",
      "Loss: 1.47503592353\n",
      "Training Accuracy: 88.945\n",
      "Epoch: 252\n",
      "Loss: 1.43048478705\n",
      "Training Accuracy: 89.4333333333\n",
      "Epoch: 253\n",
      "Loss: 1.4444213296\n",
      "Training Accuracy: 88.8983333333\n",
      "Epoch: 254\n",
      "Loss: 1.29856803503\n",
      "Training Accuracy: 88.7566666667\n",
      "Epoch: 255\n",
      "Loss: 1.37616565028\n",
      "Training Accuracy: 89.2566666667\n",
      "Epoch: 256\n",
      "Loss: 1.48520738056\n",
      "Training Accuracy: 89.26\n",
      "Epoch: 257\n",
      "Loss: 1.48268232053\n",
      "Training Accuracy: 89.0716666667\n",
      "Epoch: 258\n",
      "Loss: 1.41094817065\n",
      "Training Accuracy: 89.4216666667\n",
      "Epoch: 259\n",
      "Loss: 1.54991724016\n",
      "Training Accuracy: 89.035\n",
      "Epoch: 260\n",
      "Loss: 1.52508039583\n",
      "Training Accuracy: 88.825\n",
      "Epoch: 261\n",
      "Loss: 1.46857006328\n",
      "Training Accuracy: 88.62\n",
      "Epoch: 262\n",
      "Loss: 1.39206270638\n",
      "Training Accuracy: 88.88\n",
      "Epoch: 263\n",
      "Loss: 1.46520770078\n",
      "Training Accuracy: 88.9666666667\n",
      "Epoch: 264\n",
      "Loss: 1.36455160024\n",
      "Training Accuracy: 89.405\n",
      "Epoch: 265\n",
      "Loss: 1.4905774188\n",
      "Training Accuracy: 88.76\n",
      "Epoch: 266\n",
      "Loss: 1.39828361242\n",
      "Training Accuracy: 89.265\n",
      "Epoch: 267\n",
      "Loss: 1.39944354567\n",
      "Training Accuracy: 89.2533333333\n",
      "Epoch: 268\n",
      "Loss: 1.31552678757\n",
      "Training Accuracy: 88.77\n",
      "Epoch: 269\n",
      "Loss: 1.50106960662\n",
      "Training Accuracy: 89.8466666667\n",
      "Epoch: 270\n",
      "Loss: 1.41487566745\n",
      "Training Accuracy: 89.5133333333\n",
      "Epoch: 271\n",
      "Loss: 1.31354863075\n",
      "Training Accuracy: 89.0016666667\n",
      "Epoch: 272\n",
      "Loss: 1.44021118492\n",
      "Training Accuracy: 89.3183333333\n",
      "Epoch: 273\n",
      "Loss: 1.38699518753\n",
      "Training Accuracy: 88.8683333333\n",
      "Epoch: 274\n",
      "Loss: 1.32807951422\n",
      "Training Accuracy: 89.7333333333\n",
      "Epoch: 275\n",
      "Loss: 1.39551623111\n",
      "Training Accuracy: 89.225\n",
      "Epoch: 276\n",
      "Loss: 1.36908487049\n",
      "Training Accuracy: 89.5916666667\n",
      "Epoch: 277\n",
      "Loss: 1.48565424398\n",
      "Training Accuracy: 89.1066666667\n",
      "Epoch: 278\n",
      "Loss: 1.37356847816\n",
      "Training Accuracy: 89.7166666667\n",
      "Epoch: 279\n",
      "Loss: 1.37403291946\n",
      "Training Accuracy: 89.405\n",
      "Epoch: 280\n",
      "Loss: 1.48065812319\n",
      "Training Accuracy: 89.15\n",
      "Epoch: 281\n",
      "Loss: 1.36971786138\n",
      "Training Accuracy: 89.48\n",
      "Epoch: 282\n",
      "Loss: 1.30843489829\n",
      "Training Accuracy: 89.3633333333\n",
      "Epoch: 283\n",
      "Loss: 1.43229377519\n",
      "Training Accuracy: 88.75\n",
      "Epoch: 284\n",
      "Loss: 1.35653826969\n",
      "Training Accuracy: 89.2066666667\n",
      "Epoch: 285\n",
      "Loss: 1.3322398772\n",
      "Training Accuracy: 89.05\n",
      "Epoch: 286\n",
      "Loss: 1.42999749398\n",
      "Training Accuracy: 89.6783333333\n",
      "Epoch: 287\n",
      "Loss: 1.41252373119\n",
      "Training Accuracy: 89.755\n",
      "Epoch: 288\n",
      "Loss: 1.35934179944\n",
      "Training Accuracy: 89.6983333333\n",
      "Epoch: 289\n",
      "Loss: 1.25853728875\n",
      "Training Accuracy: 89.55\n",
      "Epoch: 290\n",
      "Loss: 1.26834909896\n",
      "Training Accuracy: 89.9266666667\n",
      "Epoch: 291\n",
      "Loss: 1.27391179905\n",
      "Training Accuracy: 89.845\n",
      "Epoch: 292\n",
      "Loss: 1.38945937115\n",
      "Training Accuracy: 89.285\n",
      "Epoch: 293\n",
      "Loss: 1.42539357086\n",
      "Training Accuracy: 89.465\n",
      "Epoch: 294\n",
      "Loss: 1.33454764337\n",
      "Training Accuracy: 89.2516666667\n",
      "Epoch: 295\n",
      "Loss: 1.28203121856\n",
      "Training Accuracy: 89.4233333333\n",
      "Epoch: 296\n",
      "Loss: 1.31199949705\n",
      "Training Accuracy: 88.8716666667\n",
      "Epoch: 297\n",
      "Loss: 1.28203564139\n",
      "Training Accuracy: 88.8266666667\n",
      "Epoch: 298\n",
      "Loss: 1.36773320519\n",
      "Training Accuracy: 89.3516666667\n",
      "Epoch: 299\n",
      "Loss: 1.33169207438\n",
      "Training Accuracy: 89.465\n",
      "Epoch: 300\n",
      "Loss: 1.35241448432\n",
      "Training Accuracy: 89.57\n",
      "Epoch: 301\n",
      "Loss: 1.31861932605\n",
      "Training Accuracy: 89.2766666667\n",
      "Epoch: 302\n",
      "Loss: 1.32329084488\n",
      "Training Accuracy: 90.0316666667\n",
      "Epoch: 303\n",
      "Loss: 1.38191313117\n",
      "Training Accuracy: 89.9433333333\n",
      "Epoch: 304\n",
      "Loss: 1.30831897393\n",
      "Training Accuracy: 89.8633333333\n",
      "Epoch: 305\n",
      "Loss: 1.42392914169\n",
      "Training Accuracy: 89.985\n",
      "Epoch: 306\n",
      "Loss: 1.31916459323\n",
      "Training Accuracy: 89.9533333333\n",
      "Epoch: 307\n",
      "Loss: 1.2235822616\n",
      "Training Accuracy: 89.6183333333\n",
      "Epoch: 308\n",
      "Loss: 1.31026356554\n",
      "Training Accuracy: 89.755\n",
      "Epoch: 309\n",
      "Loss: 1.27513081543\n",
      "Training Accuracy: 89.5416666667\n",
      "Epoch: 310\n",
      "Loss: 1.27439939709\n",
      "Training Accuracy: 89.8016666667\n",
      "Epoch: 311\n",
      "Loss: 1.27062331631\n",
      "Training Accuracy: 89.8833333333\n",
      "Epoch: 312\n",
      "Loss: 1.32667579817\n",
      "Training Accuracy: 89.9133333333\n",
      "Epoch: 313\n",
      "Loss: 1.26023533792\n",
      "Training Accuracy: 89.5\n",
      "Epoch: 314\n",
      "Loss: 1.31270228486\n",
      "Training Accuracy: 90.015\n",
      "Epoch: 315\n",
      "Loss: 1.3121642029\n",
      "Training Accuracy: 89.8233333333\n",
      "Epoch: 316\n",
      "Loss: 1.29462040478\n",
      "Training Accuracy: 90.245\n",
      "Epoch: 317\n",
      "Loss: 1.24576001328\n",
      "Training Accuracy: 90.17\n",
      "Epoch: 318\n",
      "Loss: 1.22351694569\n",
      "Training Accuracy: 89.7066666667\n",
      "Epoch: 319\n",
      "Loss: 1.35480199954\n",
      "Training Accuracy: 89.98\n",
      "Epoch: 320\n",
      "Loss: 1.35122487956\n",
      "Training Accuracy: 89.4083333333\n",
      "Epoch: 321\n",
      "Loss: 1.39825060945\n",
      "Training Accuracy: 89.6166666667\n",
      "Epoch: 322\n",
      "Loss: 1.30796047153\n",
      "Training Accuracy: 89.875\n",
      "Epoch: 323\n",
      "Loss: 1.28458602641\n",
      "Training Accuracy: 89.6366666667\n",
      "Epoch: 324\n",
      "Loss: 1.32365046184\n",
      "Training Accuracy: 89.8983333333\n",
      "Epoch: 325\n",
      "Loss: 1.25612493103\n",
      "Training Accuracy: 89.3633333333\n",
      "Epoch: 326\n",
      "Loss: 1.21959556295\n",
      "Training Accuracy: 89.9166666667\n",
      "Epoch: 327\n",
      "Loss: 1.27131887373\n",
      "Training Accuracy: 90.1816666667\n",
      "Epoch: 328\n",
      "Loss: 1.3228704021\n",
      "Training Accuracy: 90.2616666667\n",
      "Epoch: 329\n",
      "Loss: 1.28548811012\n",
      "Training Accuracy: 89.6566666667\n",
      "Epoch: 330\n",
      "Loss: 1.26219207059\n",
      "Training Accuracy: 89.635\n",
      "Epoch: 331\n",
      "Loss: 1.29787725507\n",
      "Training Accuracy: 89.8433333333\n",
      "Epoch: 332\n",
      "Loss: 1.29512153446\n",
      "Training Accuracy: 90.1083333333\n",
      "Epoch: 333\n",
      "Loss: 1.25023843836\n",
      "Training Accuracy: 90.3133333333\n",
      "Epoch: 334\n",
      "Loss: 1.28356219463\n",
      "Training Accuracy: 90.4716666667\n",
      "Epoch: 335\n",
      "Loss: 1.31535723286\n",
      "Training Accuracy: 90.21\n",
      "Epoch: 336\n",
      "Loss: 1.16986970291\n",
      "Training Accuracy: 90.025\n",
      "Epoch: 337\n",
      "Loss: 1.21248848904\n",
      "Training Accuracy: 90.0033333333\n",
      "Epoch: 338\n",
      "Loss: 1.26924794317\n",
      "Training Accuracy: 90.3366666667\n",
      "Epoch: 339\n",
      "Loss: 1.19128005956\n",
      "Training Accuracy: 90.0216666667\n",
      "Epoch: 340\n",
      "Loss: 1.21480973115\n",
      "Training Accuracy: 89.8366666667\n",
      "Epoch: 341\n",
      "Loss: 1.17431253557\n",
      "Training Accuracy: 90.0866666667\n",
      "Epoch: 342\n",
      "Loss: 1.28520823638\n",
      "Training Accuracy: 90.0166666667\n",
      "Epoch: 343\n",
      "Loss: 1.34699566395\n",
      "Training Accuracy: 90.37\n",
      "Epoch: 344\n",
      "Loss: 1.14959627434\n",
      "Training Accuracy: 90.2333333333\n",
      "Epoch: 345\n",
      "Loss: 1.23805847246\n",
      "Training Accuracy: 90.36\n",
      "Epoch: 346\n",
      "Loss: 1.21997119389\n",
      "Training Accuracy: 90.0033333333\n",
      "Epoch: 347\n",
      "Loss: 1.24100758758\n",
      "Training Accuracy: 90.4883333333\n",
      "Epoch: 348\n",
      "Loss: 1.21596600417\n",
      "Training Accuracy: 89.985\n",
      "Epoch: 349\n",
      "Loss: 1.25971249496\n",
      "Training Accuracy: 90.4283333333\n",
      "Epoch: 350\n",
      "Loss: 1.17432941435\n",
      "Training Accuracy: 89.9133333333\n",
      "Epoch: 351\n",
      "Loss: 1.21678999183\n",
      "Training Accuracy: 89.795\n",
      "Epoch: 352\n",
      "Loss: 1.18748965824\n",
      "Training Accuracy: 90.5466666667\n",
      "Epoch: 353\n",
      "Loss: 1.11765379351\n",
      "Training Accuracy: 90.2733333333\n",
      "Epoch: 354\n",
      "Loss: 1.23361490533\n",
      "Training Accuracy: 90.3483333333\n",
      "Epoch: 355\n",
      "Loss: 1.2243665187\n",
      "Training Accuracy: 90.405\n",
      "Epoch: 356\n",
      "Loss: 1.1825797505\n",
      "Training Accuracy: 90.2566666667\n",
      "Epoch: 357\n",
      "Loss: 1.17570357041\n",
      "Training Accuracy: 90.0083333333\n",
      "Epoch: 358\n",
      "Loss: 1.16538142895\n",
      "Training Accuracy: 90.1066666667\n",
      "Epoch: 359\n",
      "Loss: 1.14761248021\n",
      "Training Accuracy: 90.33\n",
      "Epoch: 360\n",
      "Loss: 1.10853357456\n",
      "Training Accuracy: 90.7416666667\n",
      "Epoch: 361\n",
      "Loss: 1.26272136788\n",
      "Training Accuracy: 90.3483333333\n",
      "Epoch: 362\n",
      "Loss: 1.1667303604\n",
      "Training Accuracy: 89.6\n",
      "Epoch: 363\n",
      "Loss: 1.25048260999\n",
      "Training Accuracy: 90.28\n",
      "Epoch: 364\n",
      "Loss: 1.15917586377\n",
      "Training Accuracy: 90.6983333333\n",
      "Epoch: 365\n",
      "Loss: 1.29693893513\n",
      "Training Accuracy: 89.9316666667\n",
      "Epoch: 366\n",
      "Loss: 1.10441824681\n",
      "Training Accuracy: 90.7683333333\n",
      "Epoch: 367\n",
      "Loss: 1.22832395188\n",
      "Training Accuracy: 90.1416666667\n",
      "Epoch: 368\n",
      "Loss: 1.16754711758\n",
      "Training Accuracy: 90.0183333333\n",
      "Epoch: 369\n",
      "Loss: 1.2377489166\n",
      "Training Accuracy: 90.6033333333\n",
      "Epoch: 370\n",
      "Loss: 1.14798482623\n",
      "Training Accuracy: 90.6216666667\n",
      "Epoch: 371\n",
      "Loss: 1.1272868398\n",
      "Training Accuracy: 90.3066666667\n",
      "Epoch: 372\n",
      "Loss: 1.29389837795\n",
      "Training Accuracy: 90.385\n",
      "Epoch: 373\n",
      "Loss: 1.05971834159\n",
      "Training Accuracy: 90.3883333333\n",
      "Epoch: 374\n",
      "Loss: 1.11216122955\n",
      "Training Accuracy: 90.415\n",
      "Epoch: 375\n",
      "Loss: 1.27331169175\n",
      "Training Accuracy: 90.24\n",
      "Epoch: 376\n",
      "Loss: 1.18176691971\n",
      "Training Accuracy: 90.5066666667\n",
      "Epoch: 377\n",
      "Loss: 1.05907196076\n",
      "Training Accuracy: 90.6983333333\n",
      "Epoch: 378\n",
      "Loss: 0.986857402598\n",
      "Training Accuracy: 90.3616666667\n",
      "Epoch: 379\n",
      "Loss: 1.07616091581\n",
      "Training Accuracy: 90.78\n",
      "Epoch: 380\n",
      "Loss: 1.19101462327\n",
      "Training Accuracy: 90.7933333333\n",
      "Epoch: 381\n",
      "Loss: 1.18747091094\n",
      "Training Accuracy: 90.6916666667\n",
      "Epoch: 382\n",
      "Loss: 1.1290360191\n",
      "Training Accuracy: 90.915\n",
      "Epoch: 383\n",
      "Loss: 1.15663107577\n",
      "Training Accuracy: 90.9316666667\n",
      "Epoch: 384\n",
      "Loss: 1.04123780459\n",
      "Training Accuracy: 90.3683333333\n",
      "Epoch: 385\n",
      "Loss: 1.07054852541\n",
      "Training Accuracy: 90.4233333333\n",
      "Epoch: 386\n",
      "Loss: 1.17746120839\n",
      "Training Accuracy: 90.595\n",
      "Epoch: 387\n",
      "Loss: 1.03718898855\n",
      "Training Accuracy: 90.485\n",
      "Epoch: 388\n",
      "Loss: 1.13735122028\n",
      "Training Accuracy: 90.46\n",
      "Epoch: 389\n",
      "Loss: 1.00917184529\n",
      "Training Accuracy: 90.5066666667\n",
      "Epoch: 390\n",
      "Loss: 1.129504305\n",
      "Training Accuracy: 90.585\n",
      "Epoch: 391\n",
      "Loss: 1.15294780369\n",
      "Training Accuracy: 90.8033333333\n",
      "Epoch: 392\n",
      "Loss: 1.20549573575\n",
      "Training Accuracy: 90.6333333333\n",
      "Epoch: 393\n",
      "Loss: 1.18041387457\n",
      "Training Accuracy: 90.2466666667\n",
      "Epoch: 394\n",
      "Loss: 0.962557398955\n",
      "Training Accuracy: 90.33\n",
      "Epoch: 395\n",
      "Loss: 1.12790245233\n",
      "Training Accuracy: 90.8983333333\n",
      "Epoch: 396\n",
      "Loss: 1.05228594235\n",
      "Training Accuracy: 90.8116666667\n",
      "Epoch: 397\n",
      "Loss: 1.0512646371\n",
      "Training Accuracy: 90.4316666667\n",
      "Epoch: 398\n",
      "Loss: 1.23392420842\n",
      "Training Accuracy: 90.98\n",
      "Epoch: 399\n",
      "Loss: 1.07160200681\n",
      "Training Accuracy: 90.6716666667\n",
      "Epoch: 400\n",
      "Loss: 1.11728225028\n",
      "Training Accuracy: 90.7133333333\n",
      "Epoch: 401\n",
      "Loss: 1.10568480453\n",
      "Training Accuracy: 90.86\n",
      "Epoch: 402\n",
      "Loss: 1.09239445442\n",
      "Training Accuracy: 90.7483333333\n",
      "Epoch: 403\n",
      "Loss: 1.00616849778\n",
      "Training Accuracy: 90.785\n",
      "Epoch: 404\n",
      "Loss: 1.05255729082\n",
      "Training Accuracy: 90.5866666667\n",
      "Epoch: 405\n",
      "Loss: 1.16316797429\n",
      "Training Accuracy: 90.9566666667\n",
      "Epoch: 406\n",
      "Loss: 1.0781721155\n",
      "Training Accuracy: 90.7233333333\n",
      "Epoch: 407\n",
      "Loss: 1.06826103637\n",
      "Training Accuracy: 90.6866666667\n",
      "Epoch: 408\n",
      "Loss: 1.09867656206\n",
      "Training Accuracy: 91.27\n",
      "Epoch: 409\n",
      "Loss: 1.18991338398\n",
      "Training Accuracy: 90.6766666667\n",
      "Epoch: 410\n",
      "Loss: 1.0575848008\n",
      "Training Accuracy: 90.8066666667\n",
      "Epoch: 411\n",
      "Loss: 0.993276644429\n",
      "Training Accuracy: 90.8016666667\n",
      "Epoch: 412\n",
      "Loss: 1.07426560167\n",
      "Training Accuracy: 90.8383333333\n",
      "Epoch: 413\n",
      "Loss: 1.19161083019\n",
      "Training Accuracy: 91.2783333333\n",
      "Epoch: 414\n",
      "Loss: 1.02837942753\n",
      "Training Accuracy: 90.5633333333\n",
      "Epoch: 415\n",
      "Loss: 1.04595110375\n",
      "Training Accuracy: 91.1083333333\n",
      "Epoch: 416\n",
      "Loss: 1.08159979175\n",
      "Training Accuracy: 91.0783333333\n",
      "Epoch: 417\n",
      "Loss: 1.0535712097\n",
      "Training Accuracy: 90.9633333333\n",
      "Epoch: 418\n",
      "Loss: 1.04412879296\n",
      "Training Accuracy: 91.2516666667\n",
      "Epoch: 419\n",
      "Loss: 1.08507567899\n",
      "Training Accuracy: 90.95\n",
      "Epoch: 420\n",
      "Loss: 1.0822524042\n",
      "Training Accuracy: 90.8616666667\n",
      "Epoch: 421\n",
      "Loss: 1.08421697352\n",
      "Training Accuracy: 90.8016666667\n",
      "Epoch: 422\n",
      "Loss: 1.00502799009\n",
      "Training Accuracy: 90.8983333333\n",
      "Epoch: 423\n",
      "Loss: 1.03951918164\n",
      "Training Accuracy: 90.9316666667\n",
      "Epoch: 424\n",
      "Loss: 1.00332440035\n",
      "Training Accuracy: 91.0833333333\n",
      "Epoch: 425\n",
      "Loss: 1.01284419933\n",
      "Training Accuracy: 91.1266666667\n",
      "Epoch: 426\n",
      "Loss: 1.02266819808\n",
      "Training Accuracy: 90.99\n",
      "Epoch: 427\n",
      "Loss: 1.18842366159\n",
      "Training Accuracy: 91.0\n",
      "Epoch: 428\n",
      "Loss: 0.976532375156\n",
      "Training Accuracy: 90.405\n",
      "Epoch: 429\n",
      "Loss: 0.979453899231\n",
      "Training Accuracy: 90.8533333333\n",
      "Epoch: 430\n",
      "Loss: 1.04911579061\n",
      "Training Accuracy: 90.83\n",
      "Epoch: 431\n",
      "Loss: 0.97350653322\n",
      "Training Accuracy: 91.0466666667\n",
      "Epoch: 432\n",
      "Loss: 0.947227734651\n",
      "Training Accuracy: 91.2183333333\n",
      "Epoch: 433\n",
      "Loss: 0.965621621765\n",
      "Training Accuracy: 91.5266666667\n",
      "Epoch: 434\n",
      "Loss: 0.991874586206\n",
      "Training Accuracy: 91.0316666667\n",
      "Epoch: 435\n",
      "Loss: 0.996046340853\n",
      "Training Accuracy: 91.1533333333\n",
      "Epoch: 436\n",
      "Loss: 0.998264200198\n",
      "Training Accuracy: 91.2833333333\n",
      "Epoch: 437\n",
      "Loss: 0.977902960639\n",
      "Training Accuracy: 91.6266666667\n",
      "Epoch: 438\n",
      "Loss: 0.975000631574\n",
      "Training Accuracy: 91.665\n",
      "Epoch: 439\n",
      "Loss: 0.973059806361\n",
      "Training Accuracy: 91.4083333333\n",
      "Epoch: 440\n",
      "Loss: 1.04087709316\n",
      "Training Accuracy: 91.3566666667\n",
      "Epoch: 441\n",
      "Loss: 1.00881808909\n",
      "Training Accuracy: 91.525\n",
      "Epoch: 442\n",
      "Loss: 0.906488876048\n",
      "Training Accuracy: 91.4166666667\n",
      "Epoch: 443\n",
      "Loss: 0.974192918341\n",
      "Training Accuracy: 91.6083333333\n",
      "Epoch: 444\n",
      "Loss: 0.967724713342\n",
      "Training Accuracy: 91.3616666667\n",
      "Epoch: 445\n",
      "Loss: 1.12043147057\n",
      "Training Accuracy: 91.715\n",
      "Epoch: 446\n",
      "Loss: 0.955132203193\n",
      "Training Accuracy: 91.3683333333\n",
      "Epoch: 447\n",
      "Loss: 1.02835772555\n",
      "Training Accuracy: 91.1666666667\n",
      "Epoch: 448\n",
      "Loss: 1.11972286391\n",
      "Training Accuracy: 91.0816666667\n",
      "Epoch: 449\n",
      "Loss: 1.02229793908\n",
      "Training Accuracy: 91.6016666667\n",
      "Epoch: 450\n",
      "Loss: 1.04468644873\n",
      "Training Accuracy: 91.285\n",
      "Epoch: 451\n",
      "Loss: 0.944512622866\n",
      "Training Accuracy: 91.585\n",
      "Epoch: 452\n",
      "Loss: 1.00860608251\n",
      "Training Accuracy: 91.685\n",
      "Epoch: 453\n",
      "Loss: 0.938552756659\n",
      "Training Accuracy: 91.5283333333\n",
      "Epoch: 454\n",
      "Loss: 0.895407697206\n",
      "Training Accuracy: 91.4166666667\n",
      "Epoch: 455\n",
      "Loss: 0.910661266249\n",
      "Training Accuracy: 91.6516666667\n",
      "Epoch: 456\n",
      "Loss: 0.962566894755\n",
      "Training Accuracy: 91.3166666667\n",
      "Epoch: 457\n",
      "Loss: 0.989750933112\n",
      "Training Accuracy: 91.7083333333\n",
      "Epoch: 458\n",
      "Loss: 1.03527026244\n",
      "Training Accuracy: 91.26\n",
      "Epoch: 459\n",
      "Loss: 0.94622987744\n",
      "Training Accuracy: 91.3216666667\n",
      "Epoch: 460\n",
      "Loss: 0.919009564309\n",
      "Training Accuracy: 91.4316666667\n",
      "Epoch: 461\n",
      "Loss: 0.998965434716\n",
      "Training Accuracy: 91.545\n",
      "Epoch: 462\n",
      "Loss: 0.902888859196\n",
      "Training Accuracy: 91.775\n",
      "Epoch: 463\n",
      "Loss: 1.02987656295\n",
      "Training Accuracy: 91.58\n",
      "Epoch: 464\n",
      "Loss: 0.949360104728\n",
      "Training Accuracy: 92.0333333333\n",
      "Epoch: 465\n",
      "Loss: 1.08210448482\n",
      "Training Accuracy: 91.5833333333\n",
      "Epoch: 466\n",
      "Loss: 1.0360140348\n",
      "Training Accuracy: 91.7516666667\n",
      "Epoch: 467\n",
      "Loss: 0.928897558222\n",
      "Training Accuracy: 91.7116666667\n",
      "Epoch: 468\n",
      "Loss: 0.930886626392\n",
      "Training Accuracy: 91.6616666667\n",
      "Epoch: 469\n",
      "Loss: 1.0294612768\n",
      "Training Accuracy: 91.4583333333\n",
      "Epoch: 470\n",
      "Loss: 0.939159021033\n",
      "Training Accuracy: 91.6916666667\n",
      "Epoch: 471\n",
      "Loss: 0.941825045225\n",
      "Training Accuracy: 91.59\n",
      "Epoch: 472\n",
      "Loss: 0.959120854629\n",
      "Training Accuracy: 91.5416666667\n",
      "Epoch: 473\n",
      "Loss: 0.964871708116\n",
      "Training Accuracy: 91.7983333333\n",
      "Epoch: 474\n",
      "Loss: 0.910360466399\n",
      "Training Accuracy: 91.87\n",
      "Epoch: 475\n",
      "Loss: 0.994323623706\n",
      "Training Accuracy: 91.6666666667\n",
      "Epoch: 476\n",
      "Loss: 0.969673532506\n",
      "Training Accuracy: 91.73\n",
      "Epoch: 477\n",
      "Loss: 0.95130716401\n",
      "Training Accuracy: 91.7183333333\n",
      "Epoch: 478\n",
      "Loss: 0.952835083948\n",
      "Training Accuracy: 91.4633333333\n",
      "Epoch: 479\n",
      "Loss: 0.923309627285\n",
      "Training Accuracy: 91.86\n",
      "Epoch: 480\n",
      "Loss: 1.02759464259\n",
      "Training Accuracy: 91.7716666667\n",
      "Epoch: 481\n",
      "Loss: 0.898359549732\n",
      "Training Accuracy: 91.8633333333\n",
      "Epoch: 482\n",
      "Loss: 0.924828185971\n",
      "Training Accuracy: 92.0366666667\n",
      "Epoch: 483\n",
      "Loss: 0.898786714065\n",
      "Training Accuracy: 91.76\n",
      "Epoch: 484\n",
      "Loss: 0.871393415606\n",
      "Training Accuracy: 91.6383333333\n",
      "Epoch: 485\n",
      "Loss: 0.909381941579\n",
      "Training Accuracy: 92.1233333333\n",
      "Epoch: 486\n",
      "Loss: 0.824631410946\n",
      "Training Accuracy: 92.0966666667\n",
      "Epoch: 487\n",
      "Loss: 1.01601937441\n",
      "Training Accuracy: 92.0266666667\n",
      "Epoch: 488\n",
      "Loss: 1.03485377741\n",
      "Training Accuracy: 91.825\n",
      "Epoch: 489\n",
      "Loss: 0.921912544946\n",
      "Training Accuracy: 92.245\n",
      "Epoch: 490\n",
      "Loss: 0.895582208691\n",
      "Training Accuracy: 91.7916666667\n",
      "Epoch: 491\n",
      "Loss: 0.91593181964\n",
      "Training Accuracy: 92.0216666667\n",
      "Epoch: 492\n",
      "Loss: 0.817485371182\n",
      "Training Accuracy: 91.8633333333\n",
      "Epoch: 493\n",
      "Loss: 0.924638758598\n",
      "Training Accuracy: 92.06\n",
      "Epoch: 494\n",
      "Loss: 0.953807254419\n",
      "Training Accuracy: 91.9733333333\n",
      "Epoch: 495\n",
      "Loss: 0.918108093907\n",
      "Training Accuracy: 92.31\n",
      "Epoch: 496\n",
      "Loss: 0.80053539917\n",
      "Training Accuracy: 92.1066666667\n",
      "Epoch: 497\n",
      "Loss: 0.89553559538\n",
      "Training Accuracy: 92.315\n",
      "Epoch: 498\n",
      "Loss: 0.795739335183\n",
      "Training Accuracy: 91.5516666667\n",
      "Epoch: 499\n",
      "Loss: 0.917460033508\n",
      "Training Accuracy: 91.8483333333\n",
      "Epoch: 500\n",
      "Loss: 0.835293788914\n",
      "Training Accuracy: 91.94\n",
      "Epoch: 501\n",
      "Loss: 0.796222280422\n",
      "Training Accuracy: 92.095\n",
      "Epoch: 502\n",
      "Loss: 0.898525817251\n",
      "Training Accuracy: 91.9866666667\n",
      "Epoch: 503\n",
      "Loss: 0.864929857602\n",
      "Training Accuracy: 92.1033333333\n",
      "Epoch: 504\n",
      "Loss: 0.865331965183\n",
      "Training Accuracy: 92.3883333333\n",
      "Epoch: 505\n",
      "Loss: 0.839247630822\n",
      "Training Accuracy: 92.3733333333\n",
      "Epoch: 506\n",
      "Loss: 0.786912703778\n",
      "Training Accuracy: 92.2166666667\n",
      "Epoch: 507\n",
      "Loss: 0.863718204802\n",
      "Training Accuracy: 92.3633333333\n",
      "Epoch: 508\n",
      "Loss: 0.834160623164\n",
      "Training Accuracy: 92.3716666667\n",
      "Epoch: 509\n",
      "Loss: 0.820292849063\n",
      "Training Accuracy: 92.2433333333\n",
      "Epoch: 510\n",
      "Loss: 0.970252598537\n",
      "Training Accuracy: 92.3583333333\n",
      "Epoch: 511\n",
      "Loss: 0.776089096168\n",
      "Training Accuracy: 92.3116666667\n",
      "Epoch: 512\n",
      "Loss: 0.875997545316\n",
      "Training Accuracy: 92.4533333333\n",
      "Epoch: 513\n",
      "Loss: 0.878214548887\n",
      "Training Accuracy: 92.2983333333\n",
      "Epoch: 514\n",
      "Loss: 0.958534907166\n",
      "Training Accuracy: 92.495\n",
      "Epoch: 515\n",
      "Loss: 0.778666626424\n",
      "Training Accuracy: 92.37\n",
      "Epoch: 516\n",
      "Loss: 0.923045713645\n",
      "Training Accuracy: 92.2483333333\n",
      "Epoch: 517\n",
      "Loss: 0.830784688259\n",
      "Training Accuracy: 91.8466666667\n",
      "Epoch: 518\n",
      "Loss: 0.847758100156\n",
      "Training Accuracy: 92.315\n",
      "Epoch: 519\n",
      "Loss: 0.856388581851\n",
      "Training Accuracy: 92.0533333333\n",
      "Epoch: 520\n",
      "Loss: 0.815902710997\n",
      "Training Accuracy: 92.0116666667\n",
      "Epoch: 521\n",
      "Loss: 0.916499660702\n",
      "Training Accuracy: 92.27\n",
      "Epoch: 522\n",
      "Loss: 0.858502005913\n",
      "Training Accuracy: 92.3516666667\n",
      "Epoch: 523\n",
      "Loss: 0.701911831667\n",
      "Training Accuracy: 92.3633333333\n",
      "Epoch: 524\n",
      "Loss: 0.909542392998\n",
      "Training Accuracy: 92.38\n",
      "Epoch: 525\n",
      "Loss: 0.788792586099\n",
      "Training Accuracy: 92.4083333333\n",
      "Epoch: 526\n",
      "Loss: 0.798825933263\n",
      "Training Accuracy: 92.4766666667\n",
      "Epoch: 527\n",
      "Loss: 0.776477125627\n",
      "Training Accuracy: 92.4933333333\n",
      "Epoch: 528\n",
      "Loss: 0.794113824736\n",
      "Training Accuracy: 92.58\n",
      "Epoch: 529\n",
      "Loss: 0.888090329093\n",
      "Training Accuracy: 92.48\n",
      "Epoch: 530\n",
      "Loss: 0.763514962922\n",
      "Training Accuracy: 92.445\n",
      "Epoch: 531\n",
      "Loss: 0.88048479245\n",
      "Training Accuracy: 92.465\n",
      "Epoch: 532\n",
      "Loss: 0.911095536703\n",
      "Training Accuracy: 92.3033333333\n",
      "Epoch: 533\n",
      "Loss: 0.820502905276\n",
      "Training Accuracy: 92.58\n",
      "Epoch: 534\n",
      "Loss: 0.769601218746\n",
      "Training Accuracy: 92.6\n",
      "Epoch: 535\n",
      "Loss: 0.824199809864\n",
      "Training Accuracy: 92.4966666667\n",
      "Epoch: 536\n",
      "Loss: 0.768626494456\n",
      "Training Accuracy: 92.7766666667\n",
      "Epoch: 537\n",
      "Loss: 0.741292547515\n",
      "Training Accuracy: 92.495\n",
      "Epoch: 538\n",
      "Loss: 0.720126630888\n",
      "Training Accuracy: 92.4583333333\n",
      "Epoch: 539\n",
      "Loss: 0.778742242052\n",
      "Training Accuracy: 92.5583333333\n",
      "Epoch: 540\n",
      "Loss: 0.74912801436\n",
      "Training Accuracy: 92.7333333333\n",
      "Epoch: 541\n",
      "Loss: 0.855263303359\n",
      "Training Accuracy: 92.7633333333\n",
      "Epoch: 542\n",
      "Loss: 0.787387139092\n",
      "Training Accuracy: 92.685\n",
      "Epoch: 543\n",
      "Loss: 0.856501758428\n",
      "Training Accuracy: 92.5916666667\n",
      "Epoch: 544\n",
      "Loss: 0.760723442141\n",
      "Training Accuracy: 92.685\n",
      "Epoch: 545\n",
      "Loss: 0.801813257362\n",
      "Training Accuracy: 92.7666666667\n",
      "Epoch: 546\n",
      "Loss: 0.86113736359\n",
      "Training Accuracy: 92.365\n",
      "Epoch: 547\n",
      "Loss: 0.848022099431\n",
      "Training Accuracy: 92.7433333333\n",
      "Epoch: 548\n",
      "Loss: 0.825586781009\n",
      "Training Accuracy: 92.7983333333\n",
      "Epoch: 549\n",
      "Loss: 0.851254692307\n",
      "Training Accuracy: 92.765\n",
      "Epoch: 550\n",
      "Loss: 0.752920915427\n",
      "Training Accuracy: 92.695\n",
      "Epoch: 551\n",
      "Loss: 0.871431945015\n",
      "Training Accuracy: 92.9133333333\n",
      "Epoch: 552\n",
      "Loss: 0.758166102698\n",
      "Training Accuracy: 92.7433333333\n",
      "Epoch: 553\n",
      "Loss: 0.719786945666\n",
      "Training Accuracy: 92.7166666667\n",
      "Epoch: 554\n",
      "Loss: 0.793484521575\n",
      "Training Accuracy: 92.7666666667\n",
      "Epoch: 555\n",
      "Loss: 0.811012294709\n",
      "Training Accuracy: 92.5233333333\n",
      "Epoch: 556\n",
      "Loss: 0.823922453907\n",
      "Training Accuracy: 92.9583333333\n",
      "Epoch: 557\n",
      "Loss: 0.755155721956\n",
      "Training Accuracy: 92.985\n",
      "Epoch: 558\n",
      "Loss: 0.826613689083\n",
      "Training Accuracy: 92.7883333333\n",
      "Epoch: 559\n",
      "Loss: 0.792073536957\n",
      "Training Accuracy: 92.9916666667\n",
      "Epoch: 560\n",
      "Loss: 0.889246911139\n",
      "Training Accuracy: 93.1\n",
      "Epoch: 561\n",
      "Loss: 0.779584297435\n",
      "Training Accuracy: 93.0283333333\n",
      "Epoch: 562\n",
      "Loss: 0.852283068816\n",
      "Training Accuracy: 92.715\n",
      "Epoch: 563\n",
      "Loss: 0.780732673921\n",
      "Training Accuracy: 92.7333333333\n",
      "Epoch: 564\n",
      "Loss: 0.752933066693\n",
      "Training Accuracy: 92.755\n",
      "Epoch: 565\n",
      "Loss: 0.787356440828\n",
      "Training Accuracy: 93.0466666667\n",
      "Epoch: 566\n",
      "Loss: 0.700880671462\n",
      "Training Accuracy: 93.0566666667\n",
      "Epoch: 567\n",
      "Loss: 0.74492990249\n",
      "Training Accuracy: 93.0633333333\n",
      "Epoch: 568\n",
      "Loss: 0.750633216033\n",
      "Training Accuracy: 92.795\n",
      "Epoch: 569\n",
      "Loss: 0.734718560843\n",
      "Training Accuracy: 92.9733333333\n",
      "Epoch: 570\n",
      "Loss: 0.795391293051\n",
      "Training Accuracy: 92.9216666667\n",
      "Epoch: 571\n",
      "Loss: 0.73230036811\n",
      "Training Accuracy: 93.0766666667\n",
      "Epoch: 572\n",
      "Loss: 0.744330141114\n",
      "Training Accuracy: 93.0416666667\n",
      "Epoch: 573\n",
      "Loss: 0.718394935854\n",
      "Training Accuracy: 92.98\n",
      "Epoch: 574\n",
      "Loss: 0.629415829964\n",
      "Training Accuracy: 92.9533333333\n",
      "Epoch: 575\n",
      "Loss: 0.688431128724\n",
      "Training Accuracy: 93.1833333333\n",
      "Epoch: 576\n",
      "Loss: 0.765015819971\n",
      "Training Accuracy: 92.9733333333\n",
      "Epoch: 577\n",
      "Loss: 0.744344309251\n",
      "Training Accuracy: 93.015\n",
      "Epoch: 578\n",
      "Loss: 0.797794013875\n",
      "Training Accuracy: 93.1083333333\n",
      "Epoch: 579\n",
      "Loss: 0.737748019828\n",
      "Training Accuracy: 92.9766666667\n",
      "Epoch: 580\n",
      "Loss: 0.706750885289\n",
      "Training Accuracy: 93.105\n",
      "Epoch: 581\n",
      "Loss: 0.787031891724\n",
      "Training Accuracy: 93.2283333333\n",
      "Epoch: 582\n",
      "Loss: 0.731526627433\n",
      "Training Accuracy: 93.0733333333\n",
      "Epoch: 583\n",
      "Loss: 0.729052221808\n",
      "Training Accuracy: 93.06\n",
      "Epoch: 584\n",
      "Loss: 0.751017185052\n",
      "Training Accuracy: 92.9133333333\n",
      "Epoch: 585\n",
      "Loss: 0.658160970474\n",
      "Training Accuracy: 93.2466666667\n",
      "Epoch: 586\n",
      "Loss: 0.728392134999\n",
      "Training Accuracy: 92.9933333333\n",
      "Epoch: 587\n",
      "Loss: 0.731635344436\n",
      "Training Accuracy: 93.14\n",
      "Epoch: 588\n",
      "Loss: 0.658705446293\n",
      "Training Accuracy: 93.2433333333\n",
      "Epoch: 589\n",
      "Loss: 0.780148798494\n",
      "Training Accuracy: 93.2233333333\n",
      "Epoch: 590\n",
      "Loss: 0.784996547653\n",
      "Training Accuracy: 93.1283333333\n",
      "Epoch: 591\n",
      "Loss: 0.724499913063\n",
      "Training Accuracy: 92.9366666667\n",
      "Epoch: 592\n",
      "Loss: 0.839716050043\n",
      "Training Accuracy: 93.3666666667\n",
      "Epoch: 593\n",
      "Loss: 0.693606042787\n",
      "Training Accuracy: 93.2766666667\n",
      "Epoch: 594\n",
      "Loss: 0.61950193624\n",
      "Training Accuracy: 93.2533333333\n",
      "Epoch: 595\n",
      "Loss: 0.711860192608\n",
      "Training Accuracy: 93.0266666667\n",
      "Epoch: 596\n",
      "Loss: 0.677833374116\n",
      "Training Accuracy: 93.375\n",
      "Epoch: 597\n",
      "Loss: 0.670980175951\n",
      "Training Accuracy: 93.335\n",
      "Epoch: 598\n",
      "Loss: 0.665763415086\n",
      "Training Accuracy: 93.185\n",
      "Epoch: 599\n",
      "Loss: 0.771345506738\n",
      "Training Accuracy: 93.385\n",
      "Epoch: 600\n",
      "Loss: 0.706937341042\n",
      "Training Accuracy: 93.3583333333\n",
      "Epoch: 601\n",
      "Loss: 0.767063026569\n",
      "Training Accuracy: 93.42\n",
      "Epoch: 602\n",
      "Loss: 0.657178474084\n",
      "Training Accuracy: 93.59\n",
      "Epoch: 603\n",
      "Loss: 0.629854122914\n",
      "Training Accuracy: 93.5166666667\n",
      "Epoch: 604\n",
      "Loss: 0.689951431814\n",
      "Training Accuracy: 93.3183333333\n",
      "Epoch: 605\n",
      "Loss: 0.767898584568\n",
      "Training Accuracy: 93.4216666667\n",
      "Epoch: 606\n",
      "Loss: 0.788139238545\n",
      "Training Accuracy: 93.4466666667\n",
      "Epoch: 607\n",
      "Loss: 0.686884739283\n",
      "Training Accuracy: 93.4733333333\n",
      "Epoch: 608\n",
      "Loss: 0.682267467548\n",
      "Training Accuracy: 93.445\n",
      "Epoch: 609\n",
      "Loss: 0.63909052569\n",
      "Training Accuracy: 93.4933333333\n",
      "Epoch: 610\n",
      "Loss: 0.611075444054\n",
      "Training Accuracy: 93.3516666667\n",
      "Epoch: 611\n",
      "Loss: 0.619183247521\n",
      "Training Accuracy: 93.395\n",
      "Epoch: 612\n",
      "Loss: 0.705662963282\n",
      "Training Accuracy: 93.5683333333\n",
      "Epoch: 613\n",
      "Loss: 0.689796744099\n",
      "Training Accuracy: 93.5683333333\n",
      "Epoch: 614\n",
      "Loss: 0.702715187759\n",
      "Training Accuracy: 93.4533333333\n",
      "Epoch: 615\n",
      "Loss: 0.689808416164\n",
      "Training Accuracy: 93.56\n",
      "Epoch: 616\n",
      "Loss: 0.66339696261\n",
      "Training Accuracy: 93.6066666667\n",
      "Epoch: 617\n",
      "Loss: 0.773929437373\n",
      "Training Accuracy: 93.58\n",
      "Epoch: 618\n",
      "Loss: 0.648803966806\n",
      "Training Accuracy: 93.5716666667\n",
      "Epoch: 619\n",
      "Loss: 0.629295041392\n",
      "Training Accuracy: 93.615\n",
      "Epoch: 620\n",
      "Loss: 0.689536526403\n",
      "Training Accuracy: 93.7833333333\n",
      "Epoch: 621\n",
      "Loss: 0.55498205187\n",
      "Training Accuracy: 93.4433333333\n",
      "Epoch: 622\n",
      "Loss: 0.638130428888\n",
      "Training Accuracy: 93.8016666667\n",
      "Epoch: 623\n",
      "Loss: 0.733153912565\n",
      "Training Accuracy: 93.605\n",
      "Epoch: 624\n",
      "Loss: 0.683745832569\n",
      "Training Accuracy: 93.5083333333\n",
      "Epoch: 625\n",
      "Loss: 0.649280053193\n",
      "Training Accuracy: 93.6016666667\n",
      "Epoch: 626\n",
      "Loss: 0.719548107438\n",
      "Training Accuracy: 93.7316666667\n",
      "Epoch: 627\n",
      "Loss: 0.621046806569\n",
      "Training Accuracy: 93.64\n",
      "Epoch: 628\n",
      "Loss: 0.649833794924\n",
      "Training Accuracy: 93.685\n",
      "Epoch: 629\n",
      "Loss: 0.600204387676\n",
      "Training Accuracy: 93.5066666667\n",
      "Epoch: 630\n",
      "Loss: 0.688465905269\n",
      "Training Accuracy: 93.5366666667\n",
      "Epoch: 631\n",
      "Loss: 0.586416068128\n",
      "Training Accuracy: 93.6633333333\n",
      "Epoch: 632\n",
      "Loss: 0.689260314401\n",
      "Training Accuracy: 93.8983333333\n",
      "Epoch: 633\n",
      "Loss: 0.707971704829\n",
      "Training Accuracy: 93.6866666667\n",
      "Epoch: 634\n",
      "Loss: 0.601236789247\n",
      "Training Accuracy: 93.625\n",
      "Epoch: 635\n",
      "Loss: 0.586691082011\n",
      "Training Accuracy: 93.5466666667\n",
      "Epoch: 636\n",
      "Loss: 0.687439646724\n",
      "Training Accuracy: 93.4166666667\n",
      "Epoch: 637\n",
      "Loss: 0.609761425351\n",
      "Training Accuracy: 93.5383333333\n",
      "Epoch: 638\n",
      "Loss: 0.695726178296\n",
      "Training Accuracy: 93.7366666667\n",
      "Epoch: 639\n",
      "Loss: 0.635770307009\n",
      "Training Accuracy: 93.6116666667\n",
      "Epoch: 640\n",
      "Loss: 0.722006098769\n",
      "Training Accuracy: 93.585\n",
      "Epoch: 641\n",
      "Loss: 0.649713935833\n",
      "Training Accuracy: 93.7133333333\n",
      "Epoch: 642\n",
      "Loss: 0.630054811588\n",
      "Training Accuracy: 93.7933333333\n",
      "Epoch: 643\n",
      "Loss: 0.735963156423\n",
      "Training Accuracy: 93.755\n",
      "Epoch: 644\n",
      "Loss: 0.677621962063\n",
      "Training Accuracy: 93.6266666667\n",
      "Epoch: 645\n",
      "Loss: 0.687976604335\n",
      "Training Accuracy: 93.755\n",
      "Epoch: 646\n",
      "Loss: 0.63513665101\n",
      "Training Accuracy: 93.7766666667\n",
      "Epoch: 647\n",
      "Loss: 0.650778312048\n",
      "Training Accuracy: 93.7616666667\n",
      "Epoch: 648\n",
      "Loss: 0.6368258522\n",
      "Training Accuracy: 93.7916666667\n",
      "Epoch: 649\n",
      "Loss: 0.618644504571\n",
      "Training Accuracy: 93.6633333333\n",
      "Epoch: 650\n",
      "Loss: 0.611400630368\n",
      "Training Accuracy: 93.8216666667\n",
      "Epoch: 651\n",
      "Loss: 0.577912819377\n",
      "Training Accuracy: 93.8133333333\n",
      "Epoch: 652\n",
      "Loss: 0.582158271344\n",
      "Training Accuracy: 93.7016666667\n",
      "Epoch: 653\n",
      "Loss: 0.659512414194\n",
      "Training Accuracy: 93.895\n",
      "Epoch: 654\n",
      "Loss: 0.623925028801\n",
      "Training Accuracy: 93.8666666667\n",
      "Epoch: 655\n",
      "Loss: 0.639743337789\n",
      "Training Accuracy: 93.6833333333\n",
      "Epoch: 656\n",
      "Loss: 0.648350322312\n",
      "Training Accuracy: 93.7966666667\n",
      "Epoch: 657\n",
      "Loss: 0.644236786268\n",
      "Training Accuracy: 93.85\n",
      "Epoch: 658\n",
      "Loss: 0.630752049296\n",
      "Training Accuracy: 93.855\n",
      "Epoch: 659\n",
      "Loss: 0.599214580083\n",
      "Training Accuracy: 93.7166666667\n",
      "Epoch: 660\n",
      "Loss: 0.564202944514\n",
      "Training Accuracy: 93.7333333333\n",
      "Epoch: 661\n",
      "Loss: 0.632904853724\n",
      "Training Accuracy: 93.8066666667\n",
      "Epoch: 662\n",
      "Loss: 0.487178237386\n",
      "Training Accuracy: 93.9016666667\n",
      "Epoch: 663\n",
      "Loss: 0.632545934274\n",
      "Training Accuracy: 93.8183333333\n",
      "Epoch: 664\n",
      "Loss: 0.64668447676\n",
      "Training Accuracy: 93.9016666667\n",
      "Epoch: 665\n",
      "Loss: 0.580911058692\n",
      "Training Accuracy: 94.05\n",
      "Epoch: 666\n",
      "Loss: 0.557431914396\n",
      "Training Accuracy: 93.9633333333\n",
      "Epoch: 667\n",
      "Loss: 0.642778506243\n",
      "Training Accuracy: 94.0783333333\n",
      "Epoch: 668\n",
      "Loss: 0.592077934877\n",
      "Training Accuracy: 94.1183333333\n",
      "Epoch: 669\n",
      "Loss: 0.610721719021\n",
      "Training Accuracy: 94.105\n",
      "Epoch: 670\n",
      "Loss: 0.619197238381\n",
      "Training Accuracy: 94.0616666667\n",
      "Epoch: 671\n",
      "Loss: 0.610390625829\n",
      "Training Accuracy: 93.9016666667\n",
      "Epoch: 672\n",
      "Loss: 0.540279386191\n",
      "Training Accuracy: 94.05\n",
      "Epoch: 673\n",
      "Loss: 0.572779595738\n",
      "Training Accuracy: 94.0766666667\n",
      "Epoch: 674\n",
      "Loss: 0.587295599287\n",
      "Training Accuracy: 93.98\n",
      "Epoch: 675\n",
      "Loss: 0.58901096497\n",
      "Training Accuracy: 94.07\n",
      "Epoch: 676\n",
      "Loss: 0.554154484616\n",
      "Training Accuracy: 93.9816666667\n",
      "Epoch: 677\n",
      "Loss: 0.527118003581\n",
      "Training Accuracy: 94.2616666667\n",
      "Epoch: 678\n",
      "Loss: 0.549283379123\n",
      "Training Accuracy: 94.1366666667\n",
      "Epoch: 679\n",
      "Loss: 0.598375607854\n",
      "Training Accuracy: 93.895\n",
      "Epoch: 680\n",
      "Loss: 0.532297691115\n",
      "Training Accuracy: 94.0416666667\n",
      "Epoch: 681\n",
      "Loss: 0.580487262126\n",
      "Training Accuracy: 94.08\n",
      "Epoch: 682\n",
      "Loss: 0.6490175988\n",
      "Training Accuracy: 94.1866666667\n",
      "Epoch: 683\n",
      "Loss: 0.595661844157\n",
      "Training Accuracy: 94.2416666667\n",
      "Epoch: 684\n",
      "Loss: 0.528630972163\n",
      "Training Accuracy: 94.1483333333\n",
      "Epoch: 685\n",
      "Loss: 0.641812268844\n",
      "Training Accuracy: 94.1\n",
      "Epoch: 686\n",
      "Loss: 0.660244905546\n",
      "Training Accuracy: 94.09\n",
      "Epoch: 687\n",
      "Loss: 0.549457842506\n",
      "Training Accuracy: 94.1016666667\n",
      "Epoch: 688\n",
      "Loss: 0.626304721476\n",
      "Training Accuracy: 94.0866666667\n",
      "Epoch: 689\n",
      "Loss: 0.516942779927\n",
      "Training Accuracy: 94.03\n",
      "Epoch: 690\n",
      "Loss: 0.606009555482\n",
      "Training Accuracy: 94.0066666667\n",
      "Epoch: 691\n",
      "Loss: 0.583302374141\n",
      "Training Accuracy: 94.14\n",
      "Epoch: 692\n",
      "Loss: 0.63958593913\n",
      "Training Accuracy: 94.21\n",
      "Epoch: 693\n",
      "Loss: 0.559994539355\n",
      "Training Accuracy: 94.2066666667\n",
      "Epoch: 694\n",
      "Loss: 0.577794801401\n",
      "Training Accuracy: 94.1216666667\n",
      "Epoch: 695\n",
      "Loss: 0.566566244371\n",
      "Training Accuracy: 94.2966666667\n",
      "Epoch: 696\n",
      "Loss: 0.546843066244\n",
      "Training Accuracy: 94.08\n",
      "Epoch: 697\n",
      "Loss: 0.557287697566\n",
      "Training Accuracy: 94.3066666667\n",
      "Epoch: 698\n",
      "Loss: 0.494067848149\n",
      "Training Accuracy: 94.195\n",
      "Epoch: 699\n",
      "Loss: 0.516769059656\n",
      "Training Accuracy: 94.37\n",
      "Epoch: 700\n",
      "Loss: 0.642099818315\n",
      "Training Accuracy: 94.3083333333\n",
      "Epoch: 701\n",
      "Loss: 0.599582443496\n",
      "Training Accuracy: 94.3083333333\n",
      "Epoch: 702\n",
      "Loss: 0.575483390365\n",
      "Training Accuracy: 94.2633333333\n",
      "Epoch: 703\n",
      "Loss: 0.485482842374\n",
      "Training Accuracy: 94.275\n",
      "Epoch: 704\n",
      "Loss: 0.567489026866\n",
      "Training Accuracy: 94.4\n",
      "Epoch: 705\n",
      "Loss: 0.523857094082\n",
      "Training Accuracy: 94.3516666667\n",
      "Epoch: 706\n",
      "Loss: 0.548694120852\n",
      "Training Accuracy: 94.3516666667\n",
      "Epoch: 707\n",
      "Loss: 0.527469297085\n",
      "Training Accuracy: 94.3033333333\n",
      "Epoch: 708\n",
      "Loss: 0.538152532622\n",
      "Training Accuracy: 94.2683333333\n",
      "Epoch: 709\n",
      "Loss: 0.489480243477\n",
      "Training Accuracy: 94.26\n",
      "Epoch: 710\n",
      "Loss: 0.629653511454\n",
      "Training Accuracy: 94.49\n",
      "Epoch: 711\n",
      "Loss: 0.608147162898\n",
      "Training Accuracy: 94.5133333333\n",
      "Epoch: 712\n",
      "Loss: 0.579497397658\n",
      "Training Accuracy: 94.3766666667\n",
      "Epoch: 713\n",
      "Loss: 0.538586031121\n",
      "Training Accuracy: 94.3816666667\n",
      "Epoch: 714\n",
      "Loss: 0.499248416377\n",
      "Training Accuracy: 94.4466666667\n",
      "Epoch: 715\n",
      "Loss: 0.560937421798\n",
      "Training Accuracy: 94.4383333333\n",
      "Epoch: 716\n",
      "Loss: 0.48456646755\n",
      "Training Accuracy: 94.47\n",
      "Epoch: 717\n",
      "Loss: 0.54229879015\n",
      "Training Accuracy: 94.39\n",
      "Epoch: 718\n",
      "Loss: 0.517900696985\n",
      "Training Accuracy: 94.3183333333\n",
      "Epoch: 719\n",
      "Loss: 0.554907956351\n",
      "Training Accuracy: 94.4883333333\n",
      "Epoch: 720\n",
      "Loss: 0.468016530425\n",
      "Training Accuracy: 94.4516666667\n",
      "Epoch: 721\n",
      "Loss: 0.532983763852\n",
      "Training Accuracy: 94.38\n",
      "Epoch: 722\n",
      "Loss: 0.514753520235\n",
      "Training Accuracy: 94.4116666667\n",
      "Epoch: 723\n",
      "Loss: 0.510222121191\n",
      "Training Accuracy: 94.5116666667\n",
      "Epoch: 724\n",
      "Loss: 0.537568039058\n",
      "Training Accuracy: 94.3783333333\n",
      "Epoch: 725\n",
      "Loss: 0.541958225174\n",
      "Training Accuracy: 94.5016666667\n",
      "Epoch: 726\n",
      "Loss: 0.523391831363\n",
      "Training Accuracy: 94.4566666667\n",
      "Epoch: 727\n",
      "Loss: 0.529265176654\n",
      "Training Accuracy: 94.6\n",
      "Epoch: 728\n",
      "Loss: 0.519147530795\n",
      "Training Accuracy: 94.7066666667\n",
      "Epoch: 729\n",
      "Loss: 0.508808887079\n",
      "Training Accuracy: 94.5533333333\n",
      "Epoch: 730\n",
      "Loss: 0.576654383885\n",
      "Training Accuracy: 94.5083333333\n",
      "Epoch: 731\n",
      "Loss: 0.531489969287\n",
      "Training Accuracy: 94.61\n",
      "Epoch: 732\n",
      "Loss: 0.526881633098\n",
      "Training Accuracy: 94.5166666667\n",
      "Epoch: 733\n",
      "Loss: 0.464179545993\n",
      "Training Accuracy: 94.6066666667\n",
      "Epoch: 734\n",
      "Loss: 0.419686991885\n",
      "Training Accuracy: 94.67\n",
      "Epoch: 735\n",
      "Loss: 0.533158158744\n",
      "Training Accuracy: 94.6483333333\n",
      "Epoch: 736\n",
      "Loss: 0.489361745996\n",
      "Training Accuracy: 94.48\n",
      "Epoch: 737\n",
      "Loss: 0.542839105829\n",
      "Training Accuracy: 94.55\n",
      "Epoch: 738\n",
      "Loss: 0.522296410496\n",
      "Training Accuracy: 94.5083333333\n",
      "Epoch: 739\n",
      "Loss: 0.572880541672\n",
      "Training Accuracy: 94.5566666667\n",
      "Epoch: 740\n",
      "Loss: 0.499421821049\n",
      "Training Accuracy: 94.4816666667\n",
      "Epoch: 741\n",
      "Loss: 0.585075745299\n",
      "Training Accuracy: 94.6016666667\n",
      "Epoch: 742\n",
      "Loss: 0.551525879229\n",
      "Training Accuracy: 94.715\n",
      "Epoch: 743\n",
      "Loss: 0.602012507357\n",
      "Training Accuracy: 94.6433333333\n",
      "Epoch: 744\n",
      "Loss: 0.485757815326\n",
      "Training Accuracy: 94.6383333333\n",
      "Epoch: 745\n",
      "Loss: 0.504940939241\n",
      "Training Accuracy: 94.6716666667\n",
      "Epoch: 746\n",
      "Loss: 0.593226657548\n",
      "Training Accuracy: 94.6483333333\n",
      "Epoch: 747\n",
      "Loss: 0.555278698382\n",
      "Training Accuracy: 94.6966666667\n",
      "Epoch: 748\n",
      "Loss: 0.49880182469\n",
      "Training Accuracy: 94.53\n",
      "Epoch: 749\n",
      "Loss: 0.475496703601\n",
      "Training Accuracy: 94.5883333333\n",
      "Epoch: 750\n",
      "Loss: 0.504632551303\n",
      "Training Accuracy: 94.505\n",
      "Epoch: 751\n",
      "Loss: 0.508696368631\n",
      "Training Accuracy: 94.6433333333\n",
      "Epoch: 752\n",
      "Loss: 0.473155485831\n",
      "Training Accuracy: 94.5833333333\n",
      "Epoch: 753\n",
      "Loss: 0.499443459065\n",
      "Training Accuracy: 94.5833333333\n",
      "Epoch: 754\n",
      "Loss: 0.454327226872\n",
      "Training Accuracy: 94.6433333333\n",
      "Epoch: 755\n",
      "Loss: 0.472124205108\n",
      "Training Accuracy: 94.675\n",
      "Epoch: 756\n",
      "Loss: 0.570224260507\n",
      "Training Accuracy: 94.595\n",
      "Epoch: 757\n",
      "Loss: 0.513378316542\n",
      "Training Accuracy: 94.7233333333\n",
      "Epoch: 758\n",
      "Loss: 0.472999083716\n",
      "Training Accuracy: 94.66\n",
      "Epoch: 759\n",
      "Loss: 0.537280018894\n",
      "Training Accuracy: 94.8666666667\n",
      "Epoch: 760\n",
      "Loss: 0.486818311195\n",
      "Training Accuracy: 94.765\n",
      "Epoch: 761\n",
      "Loss: 0.471712923128\n",
      "Training Accuracy: 94.8383333333\n",
      "Epoch: 762\n",
      "Loss: 0.464456765462\n",
      "Training Accuracy: 94.69\n",
      "Epoch: 763\n",
      "Loss: 0.43189372062\n",
      "Training Accuracy: 94.8683333333\n",
      "Epoch: 764\n",
      "Loss: 0.519900282188\n",
      "Training Accuracy: 94.8216666667\n",
      "Epoch: 765\n",
      "Loss: 0.498897771804\n",
      "Training Accuracy: 94.7933333333\n",
      "Epoch: 766\n",
      "Loss: 0.526598564415\n",
      "Training Accuracy: 94.7783333333\n",
      "Epoch: 767\n",
      "Loss: 0.550354282018\n",
      "Training Accuracy: 94.7866666667\n",
      "Epoch: 768\n",
      "Loss: 0.425185811399\n",
      "Training Accuracy: 94.8\n",
      "Epoch: 769\n",
      "Loss: 0.512463311258\n",
      "Training Accuracy: 94.7516666667\n",
      "Epoch: 770\n",
      "Loss: 0.581648507835\n",
      "Training Accuracy: 94.795\n",
      "Epoch: 771\n",
      "Loss: 0.543685083797\n",
      "Training Accuracy: 94.915\n",
      "Epoch: 772\n",
      "Loss: 0.492526825148\n",
      "Training Accuracy: 94.9\n",
      "Epoch: 773\n",
      "Loss: 0.521398336974\n",
      "Training Accuracy: 94.7533333333\n",
      "Epoch: 774\n",
      "Loss: 0.471900317779\n",
      "Training Accuracy: 94.81\n",
      "Epoch: 775\n",
      "Loss: 0.590274478976\n",
      "Training Accuracy: 94.7983333333\n",
      "Epoch: 776\n",
      "Loss: 0.476504433963\n",
      "Training Accuracy: 94.5866666667\n",
      "Epoch: 777\n",
      "Loss: 0.555537384841\n",
      "Training Accuracy: 94.87\n",
      "Epoch: 778\n",
      "Loss: 0.454571177584\n",
      "Training Accuracy: 94.87\n",
      "Epoch: 779\n",
      "Loss: 0.443594684854\n",
      "Training Accuracy: 94.8833333333\n",
      "Epoch: 780\n",
      "Loss: 0.445601963306\n",
      "Training Accuracy: 94.8383333333\n",
      "Epoch: 781\n",
      "Loss: 0.522614823001\n",
      "Training Accuracy: 94.8833333333\n",
      "Epoch: 782\n",
      "Loss: 0.487523272244\n",
      "Training Accuracy: 94.765\n",
      "Epoch: 783\n",
      "Loss: 0.51783402173\n",
      "Training Accuracy: 94.8133333333\n",
      "Epoch: 784\n",
      "Loss: 0.574516758945\n",
      "Training Accuracy: 94.7983333333\n",
      "Epoch: 785\n",
      "Loss: 0.500851547639\n",
      "Training Accuracy: 94.8266666667\n",
      "Epoch: 786\n",
      "Loss: 0.501428747976\n",
      "Training Accuracy: 94.8266666667\n",
      "Epoch: 787\n",
      "Loss: 0.547706336151\n",
      "Training Accuracy: 94.91\n",
      "Epoch: 788\n",
      "Loss: 0.398076098855\n",
      "Training Accuracy: 94.8383333333\n",
      "Epoch: 789\n",
      "Loss: 0.527637905007\n",
      "Training Accuracy: 94.8383333333\n",
      "Epoch: 790\n",
      "Loss: 0.475734653418\n",
      "Training Accuracy: 94.86\n",
      "Epoch: 791\n",
      "Loss: 0.453699102355\n",
      "Training Accuracy: 94.8733333333\n",
      "Epoch: 792\n",
      "Loss: 0.445915532785\n",
      "Training Accuracy: 94.8283333333\n",
      "Epoch: 793\n",
      "Loss: 0.526510414625\n",
      "Training Accuracy: 94.7833333333\n",
      "Epoch: 794\n",
      "Loss: 0.493132938482\n",
      "Training Accuracy: 94.9016666667\n",
      "Epoch: 795\n",
      "Loss: 0.468776236361\n",
      "Training Accuracy: 94.9966666667\n",
      "Epoch: 796\n",
      "Loss: 0.481117723706\n",
      "Training Accuracy: 95.0266666667\n",
      "Epoch: 797\n",
      "Loss: 0.44386230811\n",
      "Training Accuracy: 94.9816666667\n",
      "Epoch: 798\n",
      "Loss: 0.492819187968\n",
      "Training Accuracy: 94.9366666667\n",
      "Epoch: 799\n",
      "Loss: 0.465681654932\n",
      "Training Accuracy: 94.9866666667\n",
      "Epoch: 800\n",
      "Loss: 0.46132142761\n",
      "Training Accuracy: 95.0516666667\n",
      "Epoch: 801\n",
      "Loss: 0.503806778251\n",
      "Training Accuracy: 95.0516666667\n",
      "Epoch: 802\n",
      "Loss: 0.42504072633\n",
      "Training Accuracy: 95.12\n",
      "Epoch: 803\n",
      "Loss: 0.508097936082\n",
      "Training Accuracy: 95.0366666667\n",
      "Epoch: 804\n",
      "Loss: 0.475371735685\n",
      "Training Accuracy: 94.96\n",
      "Epoch: 805\n",
      "Loss: 0.569458911589\n",
      "Training Accuracy: 95.0716666667\n",
      "Epoch: 806\n",
      "Loss: 0.470793530685\n",
      "Training Accuracy: 95.0883333333\n",
      "Epoch: 807\n",
      "Loss: 0.578981428923\n",
      "Training Accuracy: 95.07\n",
      "Epoch: 808\n",
      "Loss: 0.447773717169\n",
      "Training Accuracy: 95.005\n",
      "Epoch: 809\n",
      "Loss: 0.527240125439\n",
      "Training Accuracy: 95.1\n",
      "Epoch: 810\n",
      "Loss: 0.519854558802\n",
      "Training Accuracy: 94.9833333333\n",
      "Epoch: 811\n",
      "Loss: 0.429247426246\n",
      "Training Accuracy: 95.135\n",
      "Epoch: 812\n",
      "Loss: 0.463083511506\n",
      "Training Accuracy: 95.09\n",
      "Epoch: 813\n",
      "Loss: 0.401995490046\n",
      "Training Accuracy: 94.99\n",
      "Epoch: 814\n",
      "Loss: 0.464881177007\n",
      "Training Accuracy: 95.07\n",
      "Epoch: 815\n",
      "Loss: 0.497197259571\n",
      "Training Accuracy: 94.9966666667\n",
      "Epoch: 816\n",
      "Loss: 0.479568749545\n",
      "Training Accuracy: 95.0266666667\n",
      "Epoch: 817\n",
      "Loss: 0.446763228081\n",
      "Training Accuracy: 94.94\n",
      "Epoch: 818\n",
      "Loss: 0.431733632257\n",
      "Training Accuracy: 95.0783333333\n",
      "Epoch: 819\n",
      "Loss: 0.417510040509\n",
      "Training Accuracy: 95.1383333333\n",
      "Epoch: 820\n",
      "Loss: 0.414031554979\n",
      "Training Accuracy: 95.0416666667\n",
      "Epoch: 821\n",
      "Loss: 0.428576409783\n",
      "Training Accuracy: 95.1233333333\n",
      "Epoch: 822\n",
      "Loss: 0.391293118449\n",
      "Training Accuracy: 95.025\n",
      "Epoch: 823\n",
      "Loss: 0.428097971356\n",
      "Training Accuracy: 95.09\n",
      "Epoch: 824\n",
      "Loss: 0.484726068198\n",
      "Training Accuracy: 95.0516666667\n",
      "Epoch: 825\n",
      "Loss: 0.455182966653\n",
      "Training Accuracy: 95.1833333333\n",
      "Epoch: 826\n",
      "Loss: 0.458151237914\n",
      "Training Accuracy: 95.0516666667\n",
      "Epoch: 827\n",
      "Loss: 0.465830969526\n",
      "Training Accuracy: 95.1283333333\n",
      "Epoch: 828\n",
      "Loss: 0.451776870172\n",
      "Training Accuracy: 95.1983333333\n",
      "Epoch: 829\n",
      "Loss: 0.473900148263\n",
      "Training Accuracy: 95.1166666667\n",
      "Epoch: 830\n",
      "Loss: 0.478561665835\n",
      "Training Accuracy: 95.1916666667\n",
      "Epoch: 831\n",
      "Loss: 0.43623012076\n",
      "Training Accuracy: 95.1216666667\n",
      "Epoch: 832\n",
      "Loss: 0.403473118695\n",
      "Training Accuracy: 95.23\n",
      "Epoch: 833\n",
      "Loss: 0.483895610468\n",
      "Training Accuracy: 95.19\n",
      "Epoch: 834\n",
      "Loss: 0.401932530375\n",
      "Training Accuracy: 94.96\n",
      "Epoch: 835\n",
      "Loss: 0.424070315961\n",
      "Training Accuracy: 95.0966666667\n",
      "Epoch: 836\n",
      "Loss: 0.463830529353\n",
      "Training Accuracy: 95.135\n",
      "Epoch: 837\n",
      "Loss: 0.553220702276\n",
      "Training Accuracy: 95.135\n",
      "Epoch: 838\n",
      "Loss: 0.517617732864\n",
      "Training Accuracy: 95.2166666667\n",
      "Epoch: 839\n",
      "Loss: 0.473995403839\n",
      "Training Accuracy: 95.125\n",
      "Epoch: 840\n",
      "Loss: 0.419502085998\n",
      "Training Accuracy: 95.16\n",
      "Epoch: 841\n",
      "Loss: 0.452007982714\n",
      "Training Accuracy: 95.1416666667\n",
      "Epoch: 842\n",
      "Loss: 0.491759642253\n",
      "Training Accuracy: 95.2533333333\n",
      "Epoch: 843\n",
      "Loss: 0.395022063737\n",
      "Training Accuracy: 95.1833333333\n",
      "Epoch: 844\n",
      "Loss: 0.406016741454\n",
      "Training Accuracy: 95.195\n",
      "Epoch: 845\n",
      "Loss: 0.418988180644\n",
      "Training Accuracy: 95.2283333333\n",
      "Epoch: 846\n",
      "Loss: 0.444924424489\n",
      "Training Accuracy: 95.2466666667\n",
      "Epoch: 847\n",
      "Loss: 0.4775719854\n",
      "Training Accuracy: 95.27\n",
      "Epoch: 848\n",
      "Loss: 0.390574857516\n",
      "Training Accuracy: 95.2416666667\n",
      "Epoch: 849\n",
      "Loss: 0.435304890398\n",
      "Training Accuracy: 95.3216666667\n",
      "Epoch: 850\n",
      "Loss: 0.374051351761\n",
      "Training Accuracy: 95.36\n",
      "Epoch: 851\n",
      "Loss: 0.43875946608\n",
      "Training Accuracy: 95.33\n",
      "Epoch: 852\n",
      "Loss: 0.487256055386\n",
      "Training Accuracy: 95.285\n",
      "Epoch: 853\n",
      "Loss: 0.428420227456\n",
      "Training Accuracy: 95.1766666667\n",
      "Epoch: 854\n",
      "Loss: 0.453915995652\n",
      "Training Accuracy: 95.23\n",
      "Epoch: 855\n",
      "Loss: 0.415531667421\n",
      "Training Accuracy: 95.3366666667\n",
      "Epoch: 856\n",
      "Loss: 0.356846419594\n",
      "Training Accuracy: 95.27\n",
      "Epoch: 857\n",
      "Loss: 0.479096564884\n",
      "Training Accuracy: 95.21\n",
      "Epoch: 858\n",
      "Loss: 0.483070392811\n",
      "Training Accuracy: 95.2883333333\n",
      "Epoch: 859\n",
      "Loss: 0.403545072063\n",
      "Training Accuracy: 95.2416666667\n",
      "Epoch: 860\n",
      "Loss: 0.43949498613\n",
      "Training Accuracy: 95.2366666667\n",
      "Epoch: 861\n",
      "Loss: 0.39439330989\n",
      "Training Accuracy: 95.3\n",
      "Epoch: 862\n",
      "Loss: 0.509060508316\n",
      "Training Accuracy: 95.2733333333\n",
      "Epoch: 863\n",
      "Loss: 0.450834884959\n",
      "Training Accuracy: 95.2666666667\n",
      "Epoch: 864\n",
      "Loss: 0.360592355709\n",
      "Training Accuracy: 95.2316666667\n",
      "Epoch: 865\n",
      "Loss: 0.381671770468\n",
      "Training Accuracy: 95.3383333333\n",
      "Epoch: 866\n",
      "Loss: 0.383942622622\n",
      "Training Accuracy: 95.28\n",
      "Epoch: 867\n",
      "Loss: 0.427322070451\n",
      "Training Accuracy: 95.2483333333\n",
      "Epoch: 868\n",
      "Loss: 0.417490094889\n",
      "Training Accuracy: 95.2983333333\n",
      "Epoch: 869\n",
      "Loss: 0.402063531411\n",
      "Training Accuracy: 95.285\n",
      "Epoch: 870\n",
      "Loss: 0.469501089613\n",
      "Training Accuracy: 95.245\n",
      "Epoch: 871\n",
      "Loss: 0.446643690096\n",
      "Training Accuracy: 95.285\n",
      "Epoch: 872\n",
      "Loss: 0.458214794282\n",
      "Training Accuracy: 95.3166666667\n",
      "Epoch: 873\n",
      "Loss: 0.487960375438\n",
      "Training Accuracy: 95.2566666667\n",
      "Epoch: 874\n",
      "Loss: 0.460323031429\n",
      "Training Accuracy: 95.3533333333\n",
      "Epoch: 875\n",
      "Loss: 0.49147079906\n",
      "Training Accuracy: 95.315\n",
      "Epoch: 876\n",
      "Loss: 0.522180733464\n",
      "Training Accuracy: 95.3316666667\n",
      "Epoch: 877\n",
      "Loss: 0.43044982089\n",
      "Training Accuracy: 95.3416666667\n",
      "Epoch: 878\n",
      "Loss: 0.398506253291\n",
      "Training Accuracy: 95.3733333333\n",
      "Epoch: 879\n",
      "Loss: 0.42630680373\n",
      "Training Accuracy: 95.3166666667\n",
      "Epoch: 880\n",
      "Loss: 0.392821096413\n",
      "Training Accuracy: 95.41\n",
      "Epoch: 881\n",
      "Loss: 0.44515757378\n",
      "Training Accuracy: 95.4783333333\n",
      "Epoch: 882\n",
      "Loss: 0.412937872552\n",
      "Training Accuracy: 95.4566666667\n",
      "Epoch: 883\n",
      "Loss: 0.452032924159\n",
      "Training Accuracy: 95.4116666667\n",
      "Epoch: 884\n",
      "Loss: 0.456539284097\n",
      "Training Accuracy: 95.4216666667\n",
      "Epoch: 885\n",
      "Loss: 0.410461947365\n",
      "Training Accuracy: 95.3783333333\n",
      "Epoch: 886\n",
      "Loss: 0.40208001467\n",
      "Training Accuracy: 95.455\n",
      "Epoch: 887\n",
      "Loss: 0.418400930089\n",
      "Training Accuracy: 95.3783333333\n",
      "Epoch: 888\n",
      "Loss: 0.398961088857\n",
      "Training Accuracy: 95.3683333333\n",
      "Epoch: 889\n",
      "Loss: 0.456471675065\n",
      "Training Accuracy: 95.47\n",
      "Epoch: 890\n",
      "Loss: 0.432595376088\n",
      "Training Accuracy: 95.51\n",
      "Epoch: 891\n",
      "Loss: 0.373837224958\n",
      "Training Accuracy: 95.4933333333\n",
      "Epoch: 892\n",
      "Loss: 0.440242779177\n",
      "Training Accuracy: 95.5366666667\n",
      "Epoch: 893\n",
      "Loss: 0.495021588597\n",
      "Training Accuracy: 95.3666666667\n",
      "Epoch: 894\n",
      "Loss: 0.383287650561\n",
      "Training Accuracy: 95.545\n",
      "Epoch: 895\n",
      "Loss: 0.379282068635\n",
      "Training Accuracy: 95.4733333333\n",
      "Epoch: 896\n",
      "Loss: 0.409773509056\n",
      "Training Accuracy: 95.4966666667\n",
      "Epoch: 897\n",
      "Loss: 0.40451803187\n",
      "Training Accuracy: 95.4716666667\n",
      "Epoch: 898\n",
      "Loss: 0.40794678116\n",
      "Training Accuracy: 95.4116666667\n",
      "Epoch: 899\n",
      "Loss: 0.370939355485\n",
      "Training Accuracy: 95.4783333333\n",
      "Epoch: 900\n",
      "Loss: 0.349781765144\n",
      "Training Accuracy: 95.4216666667\n",
      "Epoch: 901\n",
      "Loss: 0.44314689112\n",
      "Training Accuracy: 95.4366666667\n",
      "Epoch: 902\n",
      "Loss: 0.366372838736\n",
      "Training Accuracy: 95.46\n",
      "Epoch: 903\n",
      "Loss: 0.426934609946\n",
      "Training Accuracy: 95.47\n",
      "Epoch: 904\n",
      "Loss: 0.397136666745\n",
      "Training Accuracy: 95.415\n",
      "Epoch: 905\n",
      "Loss: 0.350556999117\n",
      "Training Accuracy: 95.4866666667\n",
      "Epoch: 906\n",
      "Loss: 0.424807944926\n",
      "Training Accuracy: 95.455\n",
      "Epoch: 907\n",
      "Loss: 0.34231188269\n",
      "Training Accuracy: 95.4333333333\n",
      "Epoch: 908\n",
      "Loss: 0.418757417893\n",
      "Training Accuracy: 95.4183333333\n",
      "Epoch: 909\n",
      "Loss: 0.384836292067\n",
      "Training Accuracy: 95.435\n",
      "Epoch: 910\n",
      "Loss: 0.398341800012\n",
      "Training Accuracy: 95.4666666667\n",
      "Epoch: 911\n",
      "Loss: 0.421238347015\n",
      "Training Accuracy: 95.425\n",
      "Epoch: 912\n",
      "Loss: 0.337417104175\n",
      "Training Accuracy: 95.4666666667\n",
      "Epoch: 913\n",
      "Loss: 0.387119605228\n",
      "Training Accuracy: 95.4866666667\n",
      "Epoch: 914\n",
      "Loss: 0.388110478187\n",
      "Training Accuracy: 95.4633333333\n",
      "Epoch: 915\n",
      "Loss: 0.419657200935\n",
      "Training Accuracy: 95.4566666667\n",
      "Epoch: 916\n",
      "Loss: 0.369750048773\n",
      "Training Accuracy: 95.5766666667\n",
      "Epoch: 917\n",
      "Loss: 0.370978274535\n",
      "Training Accuracy: 95.545\n",
      "Epoch: 918\n",
      "Loss: 0.435109754083\n",
      "Training Accuracy: 95.5633333333\n",
      "Epoch: 919\n",
      "Loss: 0.369341990935\n",
      "Training Accuracy: 95.5216666667\n",
      "Epoch: 920\n",
      "Loss: 0.331792611938\n",
      "Training Accuracy: 95.505\n",
      "Epoch: 921\n",
      "Loss: 0.442581001985\n",
      "Training Accuracy: 95.5833333333\n",
      "Epoch: 922\n",
      "Loss: 0.492920657657\n",
      "Training Accuracy: 95.5\n",
      "Epoch: 923\n",
      "Loss: 0.330385697417\n",
      "Training Accuracy: 95.5316666667\n",
      "Epoch: 924\n",
      "Loss: 0.438986959312\n",
      "Training Accuracy: 95.585\n",
      "Epoch: 925\n",
      "Loss: 0.417799683475\n",
      "Training Accuracy: 95.4916666667\n",
      "Epoch: 926\n",
      "Loss: 0.397760477904\n",
      "Training Accuracy: 95.5616666667\n",
      "Epoch: 927\n",
      "Loss: 0.436653460799\n",
      "Training Accuracy: 95.5316666667\n",
      "Epoch: 928\n",
      "Loss: 0.355497615499\n",
      "Training Accuracy: 95.5133333333\n",
      "Epoch: 929\n",
      "Loss: 0.415408382469\n",
      "Training Accuracy: 95.5233333333\n",
      "Epoch: 930\n",
      "Loss: 0.405731019191\n",
      "Training Accuracy: 95.5333333333\n",
      "Epoch: 931\n",
      "Loss: 0.365376422473\n",
      "Training Accuracy: 95.5366666667\n",
      "Epoch: 932\n",
      "Loss: 0.400619269527\n",
      "Training Accuracy: 95.6183333333\n",
      "Epoch: 933\n",
      "Loss: 0.370250482793\n",
      "Training Accuracy: 95.5666666667\n",
      "Epoch: 934\n",
      "Loss: 0.488481667332\n",
      "Training Accuracy: 95.5616666667\n",
      "Epoch: 935\n",
      "Loss: 0.423630363948\n",
      "Training Accuracy: 95.59\n",
      "Epoch: 936\n",
      "Loss: 0.363565862329\n",
      "Training Accuracy: 95.595\n",
      "Epoch: 937\n",
      "Loss: 0.331352158146\n",
      "Training Accuracy: 95.6616666667\n",
      "Epoch: 938\n",
      "Loss: 0.36582954261\n",
      "Training Accuracy: 95.6483333333\n",
      "Epoch: 939\n",
      "Loss: 0.415610479702\n",
      "Training Accuracy: 95.61\n",
      "Epoch: 940\n",
      "Loss: 0.321129983366\n",
      "Training Accuracy: 95.6766666667\n",
      "Epoch: 941\n",
      "Loss: 0.347039576207\n",
      "Training Accuracy: 95.6016666667\n",
      "Epoch: 942\n",
      "Loss: 0.410919738249\n",
      "Training Accuracy: 95.5816666667\n",
      "Epoch: 943\n",
      "Loss: 0.380961700321\n",
      "Training Accuracy: 95.6266666667\n",
      "Epoch: 944\n",
      "Loss: 0.43684000602\n",
      "Training Accuracy: 95.6633333333\n",
      "Epoch: 945\n",
      "Loss: 0.378905409062\n",
      "Training Accuracy: 95.65\n",
      "Epoch: 946\n",
      "Loss: 0.433080557199\n",
      "Training Accuracy: 95.5816666667\n",
      "Epoch: 947\n",
      "Loss: 0.423844553701\n",
      "Training Accuracy: 95.6133333333\n",
      "Epoch: 948\n",
      "Loss: 0.380776762466\n",
      "Training Accuracy: 95.61\n",
      "Epoch: 949\n",
      "Loss: 0.429678756067\n",
      "Training Accuracy: 95.6033333333\n",
      "Epoch: 950\n",
      "Loss: 0.345779187512\n",
      "Training Accuracy: 95.6333333333\n",
      "Epoch: 951\n",
      "Loss: 0.328943168864\n",
      "Training Accuracy: 95.5633333333\n",
      "Epoch: 952\n",
      "Loss: 0.369249168066\n",
      "Training Accuracy: 95.565\n",
      "Epoch: 953\n",
      "Loss: 0.461447984568\n",
      "Training Accuracy: 95.635\n",
      "Epoch: 954\n",
      "Loss: 0.393544355395\n",
      "Training Accuracy: 95.5483333333\n",
      "Epoch: 955\n",
      "Loss: 0.365934792655\n",
      "Training Accuracy: 95.6333333333\n",
      "Epoch: 956\n",
      "Loss: 0.372681345274\n",
      "Training Accuracy: 95.6116666667\n",
      "Epoch: 957\n",
      "Loss: 0.414633132859\n",
      "Training Accuracy: 95.6116666667\n",
      "Epoch: 958\n",
      "Loss: 0.382289926814\n",
      "Training Accuracy: 95.6666666667\n",
      "Epoch: 959\n",
      "Loss: 0.496062298685\n",
      "Training Accuracy: 95.615\n",
      "Epoch: 960\n",
      "Loss: 0.411463934564\n",
      "Training Accuracy: 95.6033333333\n",
      "Epoch: 961\n",
      "Loss: 0.427734538664\n",
      "Training Accuracy: 95.61\n",
      "Epoch: 962\n",
      "Loss: 0.425394557367\n",
      "Training Accuracy: 95.6016666667\n",
      "Epoch: 963\n",
      "Loss: 0.422828157616\n",
      "Training Accuracy: 95.6283333333\n",
      "Epoch: 964\n",
      "Loss: 0.428843273821\n",
      "Training Accuracy: 95.6183333333\n",
      "Epoch: 965\n",
      "Loss: 0.383504390425\n",
      "Training Accuracy: 95.5716666667\n",
      "Epoch: 966\n",
      "Loss: 0.44359465888\n",
      "Training Accuracy: 95.65\n",
      "Epoch: 967\n",
      "Loss: 0.440071921547\n",
      "Training Accuracy: 95.6016666667\n",
      "Epoch: 968\n",
      "Loss: 0.393153691645\n",
      "Training Accuracy: 95.6266666667\n",
      "Epoch: 969\n",
      "Loss: 0.470657314312\n",
      "Training Accuracy: 95.645\n",
      "Epoch: 970\n",
      "Loss: 0.382381322464\n",
      "Training Accuracy: 95.6133333333\n",
      "Epoch: 971\n",
      "Loss: 0.395752807398\n",
      "Training Accuracy: 95.6566666667\n",
      "Epoch: 972\n",
      "Loss: 0.344586140208\n",
      "Training Accuracy: 95.6116666667\n",
      "Epoch: 973\n",
      "Loss: 0.418142635505\n",
      "Training Accuracy: 95.6666666667\n",
      "Epoch: 974\n",
      "Loss: 0.457688490535\n",
      "Training Accuracy: 95.635\n",
      "Epoch: 975\n",
      "Loss: 0.329026512238\n",
      "Training Accuracy: 95.6883333333\n",
      "Epoch: 976\n",
      "Loss: 0.396385566106\n",
      "Training Accuracy: 95.64\n",
      "Epoch: 977\n",
      "Loss: 0.378201332275\n",
      "Training Accuracy: 95.6466666667\n",
      "Epoch: 978\n",
      "Loss: 0.37831246607\n",
      "Training Accuracy: 95.6666666667\n",
      "Epoch: 979\n",
      "Loss: 0.415932450298\n",
      "Training Accuracy: 95.675\n",
      "Epoch: 980\n",
      "Loss: 0.347570708057\n",
      "Training Accuracy: 95.6433333333\n",
      "Epoch: 981\n",
      "Loss: 0.397802756122\n",
      "Training Accuracy: 95.6983333333\n",
      "Epoch: 982\n",
      "Loss: 0.375693029831\n",
      "Training Accuracy: 95.655\n",
      "Epoch: 983\n",
      "Loss: 0.385562114062\n",
      "Training Accuracy: 95.6933333333\n",
      "Epoch: 984\n",
      "Loss: 0.459569397165\n",
      "Training Accuracy: 95.7333333333\n",
      "Epoch: 985\n",
      "Loss: 0.353694086359\n",
      "Training Accuracy: 95.755\n",
      "Epoch: 986\n",
      "Loss: 0.416926091482\n",
      "Training Accuracy: 95.71\n",
      "Epoch: 987\n",
      "Loss: 0.344336671566\n",
      "Training Accuracy: 95.7183333333\n",
      "Epoch: 988\n",
      "Loss: 0.430041189469\n",
      "Training Accuracy: 95.6666666667\n",
      "Epoch: 989\n",
      "Loss: 0.317049780866\n",
      "Training Accuracy: 95.675\n",
      "Epoch: 990\n",
      "Loss: 0.451602027611\n",
      "Training Accuracy: 95.62\n",
      "Epoch: 991\n",
      "Loss: 0.362296239352\n",
      "Training Accuracy: 95.6916666667\n",
      "Epoch: 992\n",
      "Loss: 0.39487265226\n",
      "Training Accuracy: 95.6666666667\n",
      "Epoch: 993\n",
      "Loss: 0.413807122698\n",
      "Training Accuracy: 95.73\n",
      "Epoch: 994\n",
      "Loss: 0.380897782969\n",
      "Training Accuracy: 95.6533333333\n",
      "Epoch: 995\n",
      "Loss: 0.353138018089\n",
      "Training Accuracy: 95.65\n",
      "Epoch: 996\n",
      "Loss: 0.365107583536\n",
      "Training Accuracy: 95.6616666667\n",
      "Epoch: 997\n",
      "Loss: 0.346923960997\n",
      "Training Accuracy: 95.68\n",
      "Epoch: 998\n",
      "Loss: 0.383173167342\n",
      "Training Accuracy: 95.675\n",
      "Epoch: 999\n",
      "Loss: 0.379563914882\n",
      "Training Accuracy: 95.625\n",
      "Epoch: 1000\n",
      "Loss: 0.397607041169\n",
      "Training Accuracy: 95.6566666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<NeuralNetwork.NeuralNetwork at 0x10a1f8b90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, print_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ..., 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = nn.predict(X_train)\n",
    "print y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n",
      "95.795\n"
     ]
    }
   ],
   "source": [
    "print y_train.shape[0]\n",
    "print y_train_pred.shape[0]\n",
    "diffs = y_train_pred  - y_train\n",
    "count = 0.\n",
    "for i in range(y_train.shape[0]):\n",
    "    if diffs[i] != 0:\n",
    "        count = count + 1\n",
    "print 100 - count*100/y_train.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ..., 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = nn.predict(X_test)\n",
    "print y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "95.73\n",
      "(0, {3: 1, 5: 2, 6: 4, 7: 2, 8: 2})\n",
      "(1, {2: 3, 3: 2, 6: 3, 8: 5, 9: 1})\n",
      "(2, {0: 12, 3: 8, 4: 4, 6: 9, 7: 7, 8: 20, 9: 2})\n",
      "(3, {0: 1, 2: 15, 5: 10, 6: 1, 7: 11, 8: 13, 9: 2})\n",
      "(4, {0: 1, 2: 3, 6: 12, 8: 5, 9: 31})\n",
      "(5, {0: 4, 1: 1, 2: 1, 3: 11, 4: 1, 6: 13, 7: 1, 8: 11, 9: 3})\n",
      "(6, {0: 9, 1: 3, 2: 1, 4: 4, 5: 6, 8: 5})\n",
      "(7, {0: 3, 1: 4, 2: 20, 3: 4, 4: 3, 8: 5, 9: 26})\n",
      "(8, {0: 5, 1: 2, 2: 2, 3: 6, 4: 5, 5: 4, 6: 7, 7: 5, 9: 7})\n",
      "(9, {0: 7, 1: 6, 2: 1, 3: 11, 4: 14, 5: 1, 6: 1, 7: 3, 8: 9})\n"
     ]
    }
   ],
   "source": [
    "print y_test_pred.shape[0]\n",
    "print y_test.shape[0]\n",
    "diffs = y_test_pred - y_test\n",
    "mistakes = []\n",
    "count = 0.\n",
    "for i in range(y_test.shape[0]):\n",
    "    if diffs[i] != 0:\n",
    "        count = count + 1\n",
    "        mistakes.append({\"actual\": y_test[i], \"predicted\": y_test_pred[i]})\n",
    "print 100 - count*100/y_test.shape[0]\n",
    "# initialize a data structure where each item will keep track of what mispredictions there were\n",
    "mistake_categories = [(i, {k:0 for k in range(10) if k != i}) for i in range(10)]\n",
    "for mistake in mistakes:\n",
    "    mistake_categories[mistake['actual']][1][mistake['predicted']]+=1\n",
    "\n",
    "for tup in mistake_categories:\n",
    "    for k, v in tup[1].items():\n",
    "        if v == 0: del tup[1][k] # remove keys where no mistakes were found \n",
    "\n",
    "for i in range(len(mistake_categories)): print mistake_categories[i]\n",
    "# tells us our mispredictions - ex: for images with label 2, we mispredicted a 0 twelve times, and mispredicted it as an 8 twenty times. \n",
    "# print [mistakes[i] for i in range(len(mistakes)) if mistakes[i]['actual']==4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
